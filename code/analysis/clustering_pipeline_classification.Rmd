---
title: "TNBC Clustering (Gene expression, mutations, clinical data)"
author: "Victoria Rios"
date: "30/08/2022"
output:
  pdf_document: default
  html_document: default
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Analysis of TNBC gene expression information and mutations

------------------------------------------------------------------------

## Import Libraries

Avoid strings being recognized as factors and import libraries used in
the next chunks of code.
```{r}
options(stringsAsFactors = F)

# Import packages libraries
library(factoextra)
library(M3C) # for consensus clustering
library(plyr)
library(dplyr)

# Import local functions
source("~/TNBC_Research/code/general_functions.R")
```

## Import Data

### Expression data

Import resulting data frames containing gene expression data (previously
normalized with log2 and batch effect assessed by ComBat function), where each
column is a sample and each row a gene.
```{r}
# Read the csv files containing the dataframes
train <-
  read.csv("~/TNBC_Research/code/csv_and_objects/ComBat/ComBat.csv",
           row.names = "row.names")
train_mad <-
  read.csv("~/TNBC_Research/code/csv_and_objects/ComBat/ComBat_mad.csv",
           row.names = "row.names")

# Load the validation files containing the validation data frames
load("~/TNBC_Research/data/db_files/Validation/TNBC_validation_dataset.RData")

# Observe the first records of the gene expression data frame
head(train_mad)
```

### Clinical data

Import the clinical information data frame that matches with the samples
found in the genetic information data frame imported in the previous
chunks of code (specifically "train_mad").
```{r}
# Read the csv file with the clinical information
RNA_clin_data <-
  read.csv(
    "~/TNBC_Research/code/csv_and_objects/ComBat/clinical_data_combat_mad.csv",
    row.names = "row.names"
  )

# Set ID as a new column (this will be used in the Consensus Clustering section of the analysis)
RNA_clin_data$ID <- row.names(RNA_clin_data)

# Observe the first records of the clinical information data frame
head(RNA_clin_data)
```

### Validation data cleaning and transformation
```{r}
# Printing the first rows of each data frame
head(gse142258_clin)
head(gse142731_clin)
head(gse163882_clin)

# Transforming
gse142258_clin <- t(gse142258_clin)
gse142258_clin <- as.data.frame(gse142258_clin)
gse142258_clin["Sample_description"] <- lapply(gse142258_clin["Sample_description"], gsub, pattern = "-", replacement = ".")
row.names(gse142258_clin) <- gse142258_clin$Sample_description
gse142258_clin$ID <- row.names(gse142258_clin)
gse142258_clin$source_db <- "gse142258"

gse142731_clin <- t(gse142731_clin)
gse142731_clin <- as.data.frame(gse142731_clin)
row.names(gse142731_clin) <- gse142731_clin$Sample_description
gse142731_clin$ID <- row.names(gse142731_clin)
gse142731_clin$source_db <- "gse142731"

gse163882_clin <- t(gse163882_clin)
gse163882_clin <- as.data.frame(gse163882_clin)
gse163882_clin$Age <- gse163882_clin$AGE

# Remove columns using select()
gse163882_clin <- gse163882_clin %>% 
  select(-c(AGE))
row.names(gse163882_clin) <- lapply(gse163882_clin["Sample_title"], gsub, pattern = "\\:.*", replacement = "")$Sample_title
gse163882_clin$ID <- row.names(gse163882_clin)
gse163882_clin$source_db <- "gse163882"

# Merging the dataframes
RNA_clin_data_val <- merge(gse142258_clin, gse142731_clin, all = TRUE)
RNA_clin_data_val <- merge(RNA_clin_data_val, gse163882_clin, all = TRUE)
row.names(RNA_clin_data_val) <- RNA_clin_data_val$ID
```

Verify the shapes of the imported data frames.
```{r}
library(glue)

glue("Train -> rows(genes): {nrow(train)}, columns(samples): {ncol(train)}")
glue("Train_mad -> rows(genes): {nrow(train_mad)}, columns(samples): {ncol(train_mad)}")
glue("RNA_clin_data -> rows(samples): {nrow(RNA_clin_data)}, columns(clinical variables): {ncol(RNA_clin_data)}")
```

------------------------------------------------------------------------

## Data Engineering

### Clean Data

Once we have the data, we need to ensure that it is in a format that we
can use to create a model or perform statistical tests.
```{r}
# Format for the expression data
str(train_mad[,1:5])

# Format for the clinical data
str(RNA_clin_data)
```

The expression data is numeric, which is the desired format for the
analysis. For the clinical data, there are a couple modifications that
are required for the correct performance of the analysis: - age -\> int
- methastasis -\> boolean - histological_grade -\> int - tumor_stage -\>
int - chemotherapy -\> boolean - radiotherapy -\> boolean -
hormone_therapy -\> boolean - relapse_free_status -\> int - hispanic -\>
boolean - met\_\* -\> boolean - response_to_treatment -\> int (mapping)
- tabaquism -\> boolean - pathological_history -\> boolean -
family_pathological_hsitory -\> boolean - relapse -\> boolean -
TNBC_status -\> boolean

It is needed to perform feature engineering, specially on the
categorical clinical features, since some of the classes are correlated
or even duplicated among the set of unique labels per feature. A manual
renaming of the classes is performed to account for this issue.
```{r}
# Define a vector with the names of the features to engineer
features_to_engineer <- c("histological_type",
                          "histological_grade",
                          "race",
                          "age",
                          "age_at_menarche",
                          "tumor_size_mm",
                          "days_to_last_followup",
                          "chemotherapy",
                          "radiotherapy",
                          "hormone_therapy",
                          "relapse_free_status",
                          "tumor_stage",
                          "methastasis",
                          "response_to_treatment",
                          "tabaquism",
                          "hispanic",
                          "pathological_history",
                          "family_pathological_hsitory",
                          "Relapse",
                          "TNBC_status",
                          "relapse_free_status"
                          )

features_to_engineer_val <- c("Age",
                              "Stage",
                              "Race"
                              )

# Transform the clinical variables to be compliant and homogeneous
RNA_clin_data <- feature_engineering_clinical_custom(RNA_clin_data, features_to_engineer)
RNA_clin_data_val <- feature_engineering_clinical_custom(RNA_clin_data_val, features_to_engineer_val)

head(RNA_clin_data)
head(RNA_clin_data_val)
```

### Data profiling report of the clinical data

Before running the data profiling report, let's verify the columns that
could be dropped based on missing values \> 60%.
```{r}
# Load the library
library(mde)

# Get a report of the missing variables per column
missing_summary <-
  na_summary(RNA_clin_data[, !names(RNA_clin_data) %in% c("Relapse", "last_known_alive", "TNBC_status")] , sort_by = "percent_missing")
missing_summary_val <-
  na_summary(RNA_clin_data_val[, !names(RNA_clin_data_val) %in% c("Relapse", "last_known_alive", "TNBC_status")], sort_by = "percent_missing")

missing_summary
missing_summary_val
```

```{r}
# Saving the names of the variables with missing values above 75
vars_missing_above75 <- as.vector(missing_summary[missing_summary$percent_missing>=75,]$variable)
vars_missing_above75_val <- as.vector(missing_summary_val[missing_summary_val$percent_missing>=75,]$variable)

vars_missing_above75
vars_missing_above75_val
```

Drop the variables with above 75% missing data
```{r}
RNA_clin_data <-
  RNA_clin_data[, !(names(RNA_clin_data) %in% vars_missing_above75)]
RNA_clin_data_val <-
  RNA_clin_data_val[, !(
    names(RNA_clin_data_val) %in% c(
      vars_missing_above75_val,
      "Sample_geo_accession",
      "Response",
      "Sample_title",
      "Sample_description"
    )
  )]
```

Generate Data profiling report
```{r}
library(DataExplorer)
#create_report(RNA_clin_data)
#create_report(RNA_clin_data_val)
```

### Handling missing values

Print the dimensions of the clinical data set to verify if the plot will
correctly fit the screen or if it needs to be split.
```{r}
dim(RNA_clin_data)
```

Taking that into account, that is the quantity of clinical variables to
plot for a better understanding of the distribution of the missing
values in the clinical data.
```{r}
# Load the library for visualization
library(naniar)

# Plot the first half of the data
vis_miss(RNA_clin_data, sort_miss = T)
vis_miss(RNA_clin_data_val, sort_miss = T)
```

Most of the missing values seem to be aligned to similar samples, giving
an indicator of being tied to the data source, which is acceptable as
long as the resulting clusters are not influenced by the data source.

Verify the classes of the variables of interest.
```{r}
sapply(RNA_clin_data, class)
```

Separate numeric from categorical variables
```{r}
clinical_vars_names <- colnames(RNA_clin_data)
clinical_vars_names_val <- colnames(RNA_clin_data_val)

cat_vars_clin <-
  names(RNA_clin_data)[sapply(RNA_clin_data, is.character)]
cat_vars_clin_val <-
  names(RNA_clin_data_val)[sapply(RNA_clin_data_val, is.character)]
num_vars_clin <-
  c(names(RNA_clin_data)[sapply(RNA_clin_data, is.factor)], names(RNA_clin_data)[sapply(RNA_clin_data, is.numeric)])
num_vars_clin_val <-
  c(names(RNA_clin_data_val)[sapply(RNA_clin_data_val, is.factor)], names(RNA_clin_data_val)[sapply(RNA_clin_data_val, is.numeric)]) 
```

------------------------------------------------------------------------

## Dimensionality reduction

There is a great need to develop analytic methodology to analyze and
to exploit the information contained in gene expression data. Because of
the large number of genes and the complexity of biological networks,
clustering is a useful exploratory technique for analysis of gene
expression data. Prior starting the clustering process, principal
component analysis (PCA) is needed to reduce the dimensionality of our
large and complex data set and allow us to explain the data in a matrix
of fewer dimensions (more manageable and memory efficient).

### Principal components analysis (PCA)

To compute a PCA in R we use the prcomp() function. This function takes
a matrix of data, where the columns are the variables that we want to
use to transform our samples, which should be the rows of the matrix.

In this case, we look for similarities across our patients (samples =
rows) based on gene expression (variables = columns). For that reason,
we provide a transposed version of our table to the prcomp() function as
shown in the cell below:
```{r}
val_mad <- tnbc_validation_dataset

t(train_mad)[1:10,1:10]
t(val_mad)[1:10,1:10]
```

Before proceeding with the dimensionality reduction technique, we make sure that
the scale of both dataframes (train and val) are corresponding.
```{r}
# # creating a scaler based on the training set
#scaled_train = scale(t(train_mad))
# 
# # training set resulting centers and scales per gene
# train_genes_center_scale <- as.data.frame(cbind(attr(train_mad, "scaled:center"), attr(train_mad, "scaled:scale")))
# colnames(train_genes_center_scale) <- c("center","scale")
# 
#mean_center <- mean(attr(scaled_train, "scaled:center"))
#mean_scale <- mean(attr(scaled_train, "scaled:scale"))
# 
# # finding the intersecting genes between the train and validation set
# common_genes <- intersect(colnames(t(val_mad)), colnames(train_mad))
# 
# val_centers <- c()
# val_scales <- c()
# 
# for (gene in rownames(val_mad)) {
#   if (gene %in% common_genes) {
#     val_centers <- c(val_centers, train_genes_center_scale[gene,"center"])
#     val_scales <- c(val_scales, train_genes_center_scale[gene,"scale"])
#   }else{
#     val_centers <- c(val_centers, mean_center)
#     val_scales <- c(val_scales, mean_scale)
#   }
# }
# 
# # applying the scaler to the test seet
# val_mad = scale(
#   t(val_mad),
#   center = val_centers,
#   scale = val_scales
# )
```

```{r}
# train_mad <- t(train_mad)
# val_mad <- t(val_mad)
```


To generate the principal components, pass the transposed expression dataframe 
to the prcomp() function, so each row represents a sample and each column a gene.
```{r}
set.seed(123)

# Principal components analysis
x_train <- prcomp(t(train_mad), center = T, scale. = T)
x_val <- prcomp(t(val_mad), center = T, scale. = T)
```

Visualize eigenvalues (scree plot) to show the percentage of data
variance explained by each principal component. We target to keep
\~75-85% of the variance from the original dataset.
```{r}
scree_plot <- fviz_eig(x_train, addlabels=TRUE)
scree_plot_val <- fviz_eig(x_val, addlabels=TRUE)
scree_plot
scree_plot_val
```

As seen in the plot, we are not able to get the desired variance
percentage even after adding up the first 2 Principal Components (PCs),
therefore, more than 2 PCs are used for model training. On the other
side, only two PCs are used to visualize the data points in a
reader-friendly manner (Note: It is also possible to visualize the data
in 3 dimensions if required to add an additional level of explainability
to the graph).

### Looking for outliers: Graph of individuals

Plot the first two PCs to visualize the results of the dimensionality
reduction technique on the gene expression data.
```{r}
# Plot the PCA
fviz_pca_ind(
  x_train,
  col.ind = "cos2",
  # Color by the quality of representation
  gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
  repel = T,
  # Avoid text overlapping,
  max.overlaps = Inf
)

# Plot the PCA
fviz_pca_ind(
  x_val,
  col.ind = "cos2",
  # Color by the quality of representation
  gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
  repel = T,
  # Avoid text overlapping,
  max.overlaps = Inf
)
```

There are noticeable outliers present on the validation dataframe, we are going
to identify and remove them from the clinical and gene expression sets.
```{r}
# looking for outlier samples and saving their index
ind.out <-
  apply(x_val$x, 2, function(x)
    which(abs(x - mean(
      x
    )) > (5 * sd(x)))) %>%
  Reduce(union, .) %>%
  print()

# Correction of labels
RNA_clin_data_val$ID <-
  mapvalues(
    RNA_clin_data_val$ID,
    c(
      "AICK.046.(13.006)",
      "AICK.050.(13.551)",
      "AICK.066.(13.2538)",
      "AICK.097.(14.2320)"
    ),
    c(
      "AICK.046..13.006.",
      "AICK.050..13.551.",
      "AICK.066..13.2538.",
      "AICK.097..14.2320."
    )
  )

row.names(RNA_clin_data_val) <- RNA_clin_data_val$ID

# Removing the outlier samples from the dataframes
val_mad <- val_mad[,!colnames(val_mad) %in% colnames(val_mad)[ind.out]]
RNA_clin_data_val <- RNA_clin_data_val[row.names(RNA_clin_data_val) %in% colnames(val_mad),]
```

The verification of the outliers removal is visible by performing the PCA analysis
on the clean dataset.
```{r}
set.seed(123)

# Principal components analysis
x_train <- prcomp(t(train_mad), center = T, scale. = T)
x_val <- prcomp(t(val_mad), center = T, scale. = T)
```

Visualize eigenvalues (scree plot) to show the percentage of data
variance explained by each principal component. We target to keep
\~75-85% of the variance from the original dataset.
```{r}
scree_plot <- fviz_eig(x_train, addlabels=TRUE)
scree_plot_val <- fviz_eig(x_val, addlabels=TRUE)
scree_plot
scree_plot_val
```

```{r}
# Plot the PCA
fviz_pca_ind(
  x_train,
  col.ind = "cos2",
  # Color by the quality of representation
  gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
  repel = T,
  # Avoid text overlapping,
  max.overlaps = Inf
)

# Plot the PCA
fviz_pca_ind(
  x_val,
  col.ind = "cos2",
  # Color by the quality of representation
  gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
  repel = T,
  # Avoid text overlapping,
  max.overlaps = Inf
)
```
The PCA of the validation dataset confirms that the process worked.

------------------------------------------------------------------------

## Consensus Clustering

**Extract taken from the M3C package documentation
(<https://rdrr.io/bioc/M3C/f/vignettes/M3Cvignette.Rmd>)**

Genome-wide expression data is used to stratify patients into classes
using clustering algorithms for precision medicine. The Monti consensus
clustering algorithm (Monti et al., 2003) is a widely applied method to
identify the number of clusters (K) through the principle of stability
selection. This algorithm works by resampling and clustering the data
for each K and a NXN consensus matrix is calculated, where each element
represents the fraction of times two samples clustered together. A
perfectly stable matrix would consist entirely of 0s and 1s,
representing all sample pairs always clustering together or not together
over resampling iterations.

The next step is to compare the stability of these consensus matrices to
decide K. The Proportion of Ambiguous Clustering (PAC) score
(Senbabaoglu et al., 2014) has been proposed to assess consensus matrix
stability for each K, however, it has bias towards greater values of K.
This is due to a general problem with this type of consensus clustering
algorithm that occurs because as K increases the consensus matrix
converges towards a matrix of perfect stability simply by chance. The
alternative well used delta K metric to find K is subjective as it
relies on finding an elbow point and has been demonstrated to be
inferior to the PAC score.

Monte Carlo reference-based consensus clustering (M3C) (John et al.,
2018) was made to solve these problems by comparing the real stability
scores with those expected under a random model. M3C uses a Monte Carlo
simulation to generate null distributions of stability scores along the
range of K which, by comparing with the real stability scores, are used
to decide the optimal K and reject the null hypothesis K=1.

Choose the number of components that will represent \~85% of the
variance of the data.
```{r}
# Define a variable with the PCs to keep
PCs_to_keep = 285
PCs_to_keep_val = 73

# Print the cumulative explained variance of a range of PCs
summary(x_train)$importance[3, 1:PCs_to_keep][PCs_to_keep]
summary(x_val)$importance[3, 1:PCs_to_keep_val][PCs_to_keep_val]
```

Define the annotation data frame with the clinical variables to test
```{r}
library(dplyr)

# Annotation dataframe
anno_df_m3c <- RNA_clin_data[,clinical_vars_names]
anno_df_m3c_val <- RNA_clin_data_val[,clinical_vars_names_val]

# Change gene expression data type to numeric
x_train[["x"]] <-
  mutate_all(as.data.frame(x_train[["x"]]), function(x)
    as.numeric(as.character(x)))

x_val[["x"]] <-
  mutate_all(as.data.frame(x_val[["x"]]), function(x)
    as.numeric(as.character(x)))
```

### Consensus Clustering with PAC score as objective function

Perform consensus clustering on the normalized data
```{r}
library(dplyr)

# Pass the gene expression data frame to perform consensus clustering
# Where each row is a PC and each column is a sample.
# res_train <- M3C(
#   data.frame(train_mad),
#   des = anno_df_m3c,
#   seed = 123,
#   removeplots = TRUE,
#   iters = 50,
#   fsize = 8,
#   lthick = 1,
#   dotsize = 1.25
# )

# res_val <- m3c(
#   data.frame(val_mad),
#   des = anno_df_m3c_val,
#   seed = 123,
#   removeplots = true,
#   iters = 50,
#   fsize = 8,
#   lthick = 1,
#   dotsize = 1.25
# )

# Save the optimal K
# optimal_k_train = max(res_train[["assignments"]])
optimal_k_train = 2
# optimal_k_val = max(res_val[["assignments"]])
```

Also important is the relationship between the clinical variables and
the discovered clusters. In this data we want to compare with a
categorical variable so perform a chi-squared test. We are reassured to
see below K=3 is highly significant.
```{r}
# for (variable in cat_vars_clin[2:10]) {
#   temp_var <- c()
#   for (k in seq(2, 5)) {
#     myresults <- res_train$realdataresults[[k]]$ordered_annotation
#     chifit <-
#       suppressWarnings(chisq.test(table(myresults[c('consensuscluster', variable)])))
#     temp_var <- c(temp_var, round(chifit$p.value, 4))
#   }
#   print(variable)
#   print(temp_var)
# }

# for (variable in cat_vars_clin_val[2:10]) {
#   temp_var <- c()
#   for (k in seq(2, 5)) {
#     myresults <- res_val$realdataresults[[k]]$ordered_annotation
#     chifit <-
#       suppresswarnings(chisq.test(table(myresults[c('consensuscluster', variable)])))
#     temp_var <- c(temp_var, round(chifit$p.value, 4))
#   }
#   print(variable)
#   print(temp_var)
# }
```

The results of the chi-squared test for the categorical variables of
interest indicate that for K=3, the following categorical variables are
significant: - histological_type - vital_status - histological_grade -
hormone therapy - menopausal_state

(Optional) Save the result of consensus clustering for later

```{r}
## Uncomment to save the result object to a local file
# saveRDS(res_train,
#         file = "~/TNBC_Research/code/csv_and_objects/clustering/consensus_clustering_M3C_130123.rds")
# saveRDS(res_val,
#         file = "~/TNBC_Research/code/csv_and_objects/clustering/consensus_clustering_val_M3C_130123.rds")
# Restore the object
res_train <- readRDS(file = "~/TNBC_Research/code/csv_and_objects/clustering/consensus_clustering_M3C_130123.rds")
# res_val <- readRDS(file = "~/TNBC_Research/code/csv_and_objects/clustering/consensus_clustering_val_M3C_130123.rds")
```

### Display the consensus clustering scores for the selected K

The scores and p values for the PAC score objective function are
contained within the res\$scores object. We can see below the RCSI
reaches a maxima at K = 3, the p_score supports this optimal K decision.
This means the null hypothesis that K = 1 can be rejected for this data
set because we have achieved significance (alpha=0.05) versus a data set
with no clusters.

```{r}
# Consensus clustering scores
res_train$scores
# res_val$scores
```

The stability index (RCSI) is the highest for K=3, as well as it being
associated with the lowest P scores, which confirms the consensus
clustering selection.

### Visualize the consensus clustering plots

**1. CDF Plot**

In the CDF and following PAC plot we can see the inherent bias of
consensus clustering where as K increases so does the apparent stability
of the results (or CDF plot flatness), this we correct for by using a
reference. This makes the method more sensitive to detection of the
underlying structure in noisy data.
```{r}
res_train$plots[[1]]
# res_val$plots[[1]]
```

The line starts flattening or stabilizing at K=3. Showing the
performance of an optimal K.

**2. PAC Score**

This figure below shows the PAC score, we can see an elbow at K = 3
which is suggestive this is the best K. However, the bias of consensus
clustering can be seen here as the PAC score naturally tends towards
lower values as K increases (see above plot), making selecting K without
taking this into account subject to bias. Selecting the minimal PAC
score will only work when the clusters are very well separated.
```{r}
res_train$plots[[2]]
# res_val$plots[[2]]
```

**3. Relative Cluster Stability Index (RCSI)**

The Relative Cluster Stability Index (RCSI) was derived and associated
95% confidence intervals which take into account the reference PAC
scores using the reference mean. This metric is better than the PAC
score for deciding class number, where the maximum value corresponds to
the optimal K. In this example the RCSI has an optima at K=3. Either the
*RCSI* or *P scores* can be used to select K.
```{r}
res_train$plots[[3]]
# res_val$plots[[3]]
```

**4. P score**

Calculate the P score from the distribution, here we display the P
scores from the beta distribution. If none of the P scores reach
significance over a reasonable range of K (e.g. 10), then we accept the
null hypothesis. In this dataset, we can see K = 3 reaches signficance
with an alpha of 0.05, therefore we can reject the null hypothesis K=1.
```{r}
res_train$plots[[3]]
# res_val$plots[[3]]
```

After the analysis, we can be convinced that there are 3 clusters within
this data set which are not likely to have occurred by chance alone.


### Understanding M3C Outputs

The cell below extracts the ordered expression data and the ordered
annotation data from the results object after running M3C for a 3
cluster solution. We then take a look at the annotation object M3C
outputs, a consensus cluster column has been added by M3C.

Gather the RNA-seq data and clinical data from the clustering results as
different variables.
```{r}
# RNA-seq data and cluster annotation
RNA_seq_data <-
  res_train$realdataresults[[optimal_k_train]][["ordered_data"]]
# RNA_seq_data_val <-
#   res_val$realdataresults[[optimal_k_val]][["ordered_data"]]
RNA_seq_cluster_annotation <-
  res_train$realdataresults[[optimal_k_train]][["ordered_annotation"]]
# RNA_seq_cluster_annotation_val <-
#   res_val$realdataresults[[optimal_k_val]][["ordered_annotation"]]

# Joining expression and cluster annotation
RNA_seq_merged <-
  as.data.frame(t(rbind(
    RNA_seq_data, t(RNA_seq_cluster_annotation)
  )))
# RNA_seq_merged_val <-
#   as.data.frame(t(rbind(
#     RNA_seq_data_val, t(RNA_seq_cluster_annotation_val)
#   )))

RNA_seq_merged_allgenes <-
  as.data.frame(merge(t(train_mad), RNA_seq_cluster_annotation, by = "row.names"))
# RNA_seq_merged_allgenes_val <-
#   as.data.frame(merge(t(val_mad), RNA_seq_cluster_annotation_val, by = "row.names"))
RNA_seq_allgenes_val <- t(val_mad)
rownames(RNA_seq_merged_allgenes) <-
  RNA_seq_merged_allgenes$Row.names
# rownames(RNA_seq_merged_allgenes_val) <-
#   RNA_seq_merged_allgenes_val$Row.names

RNA_seq_merged_allgenes = subset(RNA_seq_merged_allgenes, select = -c(`Row.names`))
# RNA_seq_merged_allgenes_val = subset(RNA_seq_merged_allgenes_val, select = -c(`Row.names`))

# Clinical data
RNA_clin_data <-
  merge(RNA_clin_data, RNA_seq_cluster_annotation, by = "row.names")
# RNA_clin_data_val <-
#   merge(RNA_clin_data_val, RNA_seq_cluster_annotation_val, by = "row.names")

rownames(RNA_clin_data) <- RNA_clin_data$Row.names
# rownames(RNA_clin_data_val) <- RNA_clin_data_val$Row.names
```

Remove any possible duplicate column
```{r}
library(stringr)

RNA_clin_data <- RNA_clin_data %>% 
  rename_at(
    vars(ends_with(".x")),
    ~str_replace(., "\\..$","")
  ) %>% 
  select_at(
    vars(-ends_with(".y"))
  )

# RNA_clin_data_val <- RNA_clin_data_val %>% 
#   rename_at(
#     vars(ends_with(".x")),
#     ~str_replace(., "\\..$","")
#   ) %>% 
#   select_at(
#     vars(-ends_with(".y"))
#   )
```

Save the results of the 3 clusters in a set of variables
```{r}
# Training
data_train <-
  res_train$realdataresults[[optimal_k_train]]$ordered_data
annon_train <-
  res_train$realdataresults[[optimal_k_train]]$ordered_annotation
ccmatrix_train <-  res_train$realdataresults[[optimal_k_train]]$consensus_matrix

# Creating a PCs + consensusclusters annotation dataframe
data_train_consensus_clusters <- as.data.frame(t(as.data.frame(data_train)))
data_train_consensus_clusters$consensuscluster <- res_train$realdataresults[[optimal_k_train]]$ordered_annotation$consensuscluster

# Validation
# data_val <-
#   res_val$realdataresults[[optimal_k_val]]$ordered_data
# annon_val <-
#   res_val$realdataresults[[optimal_k_val]]$ordered_annotation
# ccmatrix_val <-
#   res_val$realdataresults[[optimal_k_val]]$consensus_matrix

# Creating a PCs + consensusclusters annotation dataframe
# data_val_consensus_clusters <- as.data.frame(t(as.data.frame(data_val)))
# data_val_consensus_clusters$consensuscluster <- res_val$realdataresults[[optimal_k_val]]$ordered_annotation$consensuscluster
```

Consensus matrix heatmaps from M3C output with ComplexHeatmap.
```{r}
library(ComplexHeatmap)

ccl <- consensus_complex_heatmap(res_train, optimal_k_train)
print(ccl)
```

```{r}
# ccl_val <- consensus_complex_heatmap(res_val, optimal_k_val)
# print(ccl_val)
```

From looking at the heatmap, we can discard that the clustering was
biased by the batch effect of the data sources.

## DEGs (Differentially expressed genes among clusters)

TODO: Check the final results of the clusters expressions vs the
resulting DEGs, we expect to have maximum one NA per row.
```{r}
library(limma)

design <- model.matrix( ~ 0 + RNA_clin_data$consensuscluster)
design[0:10, ]
```

```{r}
## the column names are a bit ugly, so we will rename
colnames(design) <- c("Cluster1", "Cluster2")
design[0:10, ]
```

It has been demonstrated that our power to detect differential
expression can be improved if we filter lowly-expressed genes prior to
performing the analysis. Quite how one defines a gene being expressed
may vary from experiment to experiment, so a cut-off that will work for
all datasets is not feasible. Here we consider that around 50% of our
genes will not be expressed, and use the median expression level as a
cut-off.
```{r}
RNA_seq_exprs <-
  RNA_seq_merged_allgenes[which(rownames(RNA_seq_merged_allgenes) != "consensuscluster"),]

RNA_seq_exprs <- RNA_seq_exprs[,!names(RNA_seq_exprs) %in% c(clinical_vars_names, clinical_vars_names_val, "Surv_months")]

RNA_seq_exprs <-
  mutate_all(data.frame(RNA_seq_exprs), function(x)
    as.numeric(as.character(x)))

summary(RNA_seq_exprs[, 1:10])

## calculate median expression level
temp_median <- apply(RNA_seq_exprs, 2, median)
cutoff <- median(temp_median, na.rm = T)

## TRUE or FALSE for whether each gene is "expressed" in each sample
is_expressed <- RNA_seq_exprs > cutoff

## Identify genes expressed in more than 2 samples
keep <- colSums(is_expressed) > 2

## check how many genes are removed / retained.
table(keep)

## subset to just those expressed genes
RNA_seq_exprs <- RNA_seq_exprs[,row.names(as.data.frame(keep))]
```

The lmFit function is used to fit the model to the data. The result of
which is to estimate the expression level in each of the groups that we
specified.
```{r}
RNA_seq_exprs_no_consensuscluster <-
  t(RNA_seq_exprs)[!(row.names(t(RNA_seq_exprs)) %in% c("consensuscluster")), ]

## calculate relative array weights
aw <- arrayWeights(RNA_seq_exprs_no_consensuscluster, design)
aw[0:10]

fit <-
  limma::lmFit(RNA_seq_exprs_no_consensuscluster, design, weights = aw)
fit$coefficients[1:10, ]
```

In order to perform the differential analysis, we have to define the
contrast that we are interested in. In our case we only have two groups
and one contrast of interest. Multiple contrasts can be defined in the
makeContrasts function.
```{r}
contrasts_c1c2 <- makeContrasts(Cluster1 - Cluster2, levels = design)

fitc1c2 <- contrasts.fit(fit, contrasts_c1c2)
```

Finally, apply the empirical Bayes' step to get our differential
expression statistics and p-values.
```{r}
fitc1c2 <- eBayes(fitc1c2)
```

We usually get our first look at the results by using the topTable
command
```{r}
topTable(fitc1c2)
```

If we want to know how many genes are differentially-expressed overall
we can use the decideTests function.

\*\* Cluster1 - Cluster 2
```{r}
decideTests(fitc1c2)
```

```{r}
table(decideTests(fitc1c2))
```

### Further processing and visualisation of DE results

At the moment our results are not particularly easy to navigate as the
only information to identify each gene is the identifier that the
microarray manufacturer has assigned. Fortunately, the GEO entry
contains extensive annotation that we can add. The annotation data can
be retrieved with the fData function and we restrict to columns we are
interested in using select.

For your own data, you will have to choose the columns that are of
interest to you. You probably won't have the same column headings used
here.

Once an annotation data frame has been created, it can be assigned to
our results.
```{r}
full_resultsc1c2 <- topTable(fitc1c2, number = Inf)
full_resultsc1c2 <- tibble::rownames_to_column(full_resultsc1c2, "ID")
```

The "Volcano Plot" function is a common way of visualising the results
of a DE analysis. The x axis shows the log-fold change and the y axis is
some measure of statistical significance, which in this case is the
log-odds, or "B" statistic. A characteristic "volcano" shape should be
seen.

First we create a data frame that we can visualise in ggplot2.
Specifying the number argument to topTable creates a table containing
test results from all genes. We also put the probe IDs as a column
rather than row names.

The flexibility of ggplot2 allows us to automatically label points on
the plot that might be of interest. For example, genes that meet a
particular p-value and log fold-change cut-off. With the code below the
values of p_cutoff and fc_cutoff can be changed as desired.

Furthermore, we can label the identity of some genes. Below we set a
limit of the top "N" genes we want to label, and label each gene
according to it's Symbol.
```{r}
library(ggrepel)

## change according to your needs
p_cutoff <- 0.001
fc_cutoff <- 2
topN <- 20
```

Cluster 1 vs Cluster 2
```{r}
full_resultsc1c2 %>%
  mutate(Significant = p.adjust(P.Value, method="fdr") < p_cutoff, abs(logFC) > fc_cutoff) %>%
  mutate(Rank = 1:n(), Label = ifelse(Rank < topN, ID, "")) %>%
  ggplot(aes(
    x = logFC,
    y = -log10(P.Value),
    col = Significant,
    label = Label
  )) + geom_point() + geom_text_repel(col = "black")
```

We can filter according to p-value (adjusted) and fold-change cut-offs
```{r}
sign_genes_c1c2 <- filter(full_resultsc1c2, adj.P.Val < p_cutoff)
```

We save the results
```{r}
library(readr)
full_resultsc1c2 %>%
  write_csv(file="complete_DEGs_results_c1c2_13_jan_23.csv")
```

Comparing the deferentially expressed genes between all the clusters
```{r}
significant_genes12 <- filter(full_resultsc1c2, p.adjust(P.Value, method="fdr") < p_cutoff)$ID

significant_genes <- unique(significant_genes12)

cc1 <- colMeans(RNA_seq_exprs[RNA_seq_exprs$consensuscluster == 1])
cc2 <- colMeans(RNA_seq_exprs[RNA_seq_exprs$consensuscluster == 2])

temp <-
  cbind(cc1[significant_genes], cc2[significant_genes])

rownames(temp) <- significant_genes
colnames(temp) <- c("cluster1", "cluster2")

temp
```

### Heatmaps of selected genes

**Most differentially-expressed genes** We have already created a table
of differential expression results, which is ranked according to
statistical significance.

To visualise the most differentially-expressed genes, we first need to
extract their ID. These IDs should correspond to rows in the expression
matrix.

In the code below we introduce a new column to the results which just
gives a row number to each gene. We then filter to return data for the
top N results. The pull function is used to extract the ID column as a
variable.
```{r}
## Use to top 20 genes for illustration
topN <- 20

##
ids_of_interest12 <- mutate(full_resultsc1c2, Rank = 1:n()) %>%
  filter(Rank < topN) %>%
  pull(ID)
```

In order to label the heatmap in a useful manner we extract the
corresponding gene symbols.
```{r}
gene_names12 <- mutate(full_resultsc1c2, Rank = 1:n()) %>% 
  filter(Rank < topN) %>% 
  pull(ID)
```

The expression values for the IDs we have retrieved can be obtained by
using the [..] notation to index the expression matrix.
```{r}
## Get the rows corresponding to ids_of_interest and all columns
gene_matrix12 <- t(RNA_seq_exprs)[ids_of_interest12,]
```

We now make the heatmap. A default colour scheme is used, but can be
changed via the arguments.
```{r}
pheatmap(gene_matrix12[,1:20],
     labels_row = gene_names12,
     scale="row")
```


## Classification model

Split the original dataset into stratified train and test sets. The stratification will be made based on the cluster column.
```{r}
# First, install and load the randomForest package
library(randomForest)
library(splitTools)

## Split the data into a training set and a test set

# First, we identify the intersecting genes between the training and validation gene datasets, we keep only the common genes
common_cols_train_val <- intersect(colnames(RNA_seq_merged_allgenes), colnames(RNA_seq_allgenes_val))

# We filter to keep only the DEGs found in the training dataset
common_cols_train_val_DEGs <- intersect(common_cols_train_val, significant_genes)

classif_train <-
  RNA_seq_merged_allgenes[, colnames(RNA_seq_merged_allgenes) %in% c(common_cols_train_val_DEGs, "consensuscluster")]
classif_val <- RNA_seq_allgenes_val[, colnames(RNA_seq_allgenes_val) %in% c(common_cols_train_val_DEGs)]

# Now, we remove the clinical information to generate the classification model based on gene expression
classif_train_genes <-
  classif_train[, !colnames(classif_train) %in% c(clinical_vars_names, clinical_vars_names_val, "Surv_months")]
classif_val <- classif_val[,!colnames(classif_val) %in% c(clinical_vars_names, clinical_vars_names_val, "Surv_months")]

# Split data into partitions
set.seed(123)
classif_train_split <-
  splitTools::partition(classif_train_genes$consensuscluster,
                        p = c(train = 0.8, test = 0.2))
# We split the origial dataframe based on the split recommendation
train_split <- classif_train_genes[classif_train_split$train,]
test_split <- classif_train_genes[classif_train_split$test,]

# We verify the quantity of samples per cluster of each split
table(train_split$consensuscluster)
table(test_split$consensuscluster)
```

Performing Grid Search to adjust the huperparameters to get the model that best
fits our training dataset.
```{r}
library(caret)

train_data <- train_split[,!train_split %in% c('consensuscluster')]
train_labels <- train_split$consensuscluster

# perform the tuning
set.seed(123)
tuning_results <- tuneRF(x = train_data, y = train_labels, improve = 0.57, trace = TRUE, plot = TRUE, doBest = TRUE)

# print the best hyperparameters
print(tuning_results)
```

Train the Random Forest Classification model with the best hyperparameters values
```{r}
# Train the random forest
rf_model <- randomForest(consensuscluster ~ ., data = train_split, ntree = tuning_results$ntree, mtry = tuning_results$mtry)

# Make predictions on the test set
predictions <- predict(rf_model, newdata = test_split)

# Evaluate the performance of the model
library(caret)
confusionMatrix(predictions, test_split$consensuscluster)
```


#### Classifying the validation dataset
```{r}
# Make predictions on the test set
predictions_val <- as.data.frame(predict(rf_model, newdata = classif_val))
colnames(predictions_val) <- c("predicted_clusters")

head(predictions_val)
```

Adding the cluster annotation to the total validation genes data frame
```{r}
# Genes and clustering
RNA_seq_merged_allgenes_val <-
  merge(t(val_mad), predictions_val, by = "row.names", all.x = TRUE)
rownames(RNA_seq_merged_allgenes_val) <-
  t(RNA_seq_merged_allgenes_val["Row.names"])
RNA_seq_merged_allgenes_val = subset(RNA_seq_merged_allgenes_val, select = -c(`Row.names`))

# PCs and clustering
PCs_val <- merge(x_val[["x"]][,1:PCs_to_keep_val], predictions_val, by = "row.names", all.x = TRUE)
rownames(PCs_val) <- t(PCs_val["Row.names"])
PCs_val = subset(PCs_val, select = -c(`Row.names`))

# Clinical variables
RNA_clin_data_val <-
  merge(RNA_clin_data_val, predictions_val, by = "row.names")
rownames(RNA_clin_data_val) <-
  t(RNA_clin_data_val["Row.names"])
RNA_clin_data_val = subset(RNA_clin_data_val, select = -c(`Row.names`))
```

### Plotting the clusters: training and validation datasets

**Training dataset: Consensus clusters**

Plotting Consensus Clustering results
```{r}
library(ggpubr)
library(dplyr)

# Add the cluster annotation to the training PCs dataframe
PCs_train_clusters <-
  as.data.frame(merge(x_train[["x"]], RNA_seq_cluster_annotation, by = "row.names"))
rownames(PCs_train_clusters) <-
  PCs_train_clusters$Row.names
PCs_train_clusters = subset(PCs_train_clusters, select = -c(`Row.names`))


# Add clusters obtained using the K-means algorithm for validation
x_train$cluster <- factor(RNA_clin_data$cluster)
eigenvalue_train <- round(get_eigenvalue(x_train), 1)
variance.percent_train <- eigenvalue_train$variance.percent

# Training
ggscatter(
    PCs_train_clusters,
    x = "PC1",
    y = "PC2",
    color = "consensuscluster",
    palette = "npg",
    ellipse = TRUE,
    ellipse.type = "convex",
    size = 1.5,
    legend = "right",
    ggtheme = theme_bw(),
    xlab = paste0("Dim 1 (", variance.percent_train[1], "% )"),
    ylab = paste0("Dim 2 (", variance.percent_train[2], "% )")
) +
    stat_mean(aes(color = "consensuscluster"), size = 4)

# Validation
# ggscatter(
#     data_val_consensus_clusters,
#     x = "PC1",
#     y = "PC2",
#     color = "consensuscluster",
#     palette = "npg",
#     ellipse = TRUE,
#     ellipse.type = "convex",
#     size = 1.5,
#     legend = "right",
#     ggtheme = theme_bw(),
#     xlab = paste0("Dim 1 (", variance.percent_val[1], "% )"),
#     ylab = paste0("Dim 2 (", variance.percent_val[2], "% )")
# ) +
#     stat_mean(aes(color = "consensuscluster"), size = 4)
```

Validation dataset: Classification labels
```{r}
library(ggpubr)
library(dplyr)

# Add clusters obtained using the K-means algorithm for validation
x_val$cluster <- factor(RNA_clin_data_val$cluster)
eigenvalue_val <- round(get_eigenvalue(x_val), 1)
variance.percent_val <- eigenvalue_val$variance.percent

# Validation
ggscatter(
    PCs_val,
    x = "PC1",
    y = "PC2",
    color = "predicted_clusters",
    palette = "npg",
    ellipse = TRUE,
    ellipse.type = "convex",
    size = 1.5,
    legend = "right",
    ggtheme = theme_bw(),
    xlab = paste0("Dim 1 (", variance.percent_val[1], "% )"),
    ylab = paste0("Dim 2 (", variance.percent_val[2], "% )")
) +
    stat_mean(aes(color = "predicted_clusters"), size = 4)
```





### Option 1: Using BST
Multi-class HingeBoost is applied for 200 boosting iterations. Plot the evolution
of the misclassification error on the test data versus the iteration counter, as
the multi-class HingeBoost algorithm proceeds while working on the test set.
```{r}
library(bst)

train_classif$cluster <- as.numeric(train_classif$cluster)
test_classif$V630 <- as.numeric(test_classif$V630)

m1 <- m2 <- 200
dat.m1 <- mhingebst(x=train_classif, y=train_classif$cluster, ctrl = bst_control(mstop=m1), family = "hinge", learner = c("ls"))

risk.te1 <- predict(dat.m1, newdata=test_classif, newy=test_classif$V630, mstop=m1, type="error")
plot(risk.te1, type="l", xlab="Iteration", ylab="Test Error")
```
Then, we plot the evolution of the number of genes selected versus the iteration counter.
```{r}
plot(nsel(dat.m1, m1), ylab="Number of PCs", xlab="Iteration", lty="solid", type="l")
```

Multi-class twin HingeBoost is applied based on the results from the first
round HingeBoost for 150 iterations. Plot the evolution of the misclassification error on the test data versus the iteration counter, as the multi-class twin
HingeBoost algorithm proceeds while working on the test set.
```{r}
fhat1 <- predict(dat.m1, mstop=150, type="response")
xinit <- unlist(dat.m1$ensemble[1:150])
xinit <- subset(xinit, !is.na(xinit))

### How many genes selected with mstop=150
length(unique(xinit))
dat.m2 <- mhingebst(x=train_classif, y=train_classif$cluster, ctrl = bst_control(mstop=m2,
twinboost=TRUE, f.init=fhat1, xselect.init=xinit), family = "hinge", learner = "ls")
risk.te1 <- predict(dat.m2, newdata=test_classif, newy=test_classif$V630, mstop=m2, type="error")
plot(risk.te1, type="l", xlab="Iteration", ylab="Test Error")
```
Plot the evolution of the number of genes selected versus the iteration
counter, as the multi-class twin HingeBoost algorithm proceeds while working on the training set.
```{r}
plot(nsel(dat.m2, m2), ylab="Number of PCs", xlab="Iteration", lty="solid", type="l")
```

### Option 2: Using XGBoost

Load packages
```{r}
library("xgboost")  # the main algorithm
library("archdata") # for the sample dataset
library("caret")    # for the confusionmatrix() function (also needs e1071 package)
library("dplyr")    # for some data preperation
library("Ckmeans.1d.dp") # for xgb.ggplot.importance
```

**Preprocess training and test data to feed the XGBoost model**
```{r}
# split train data and make xgb.DMatrix
train_data   <- train_classif_orig[, !names(train_classif_orig) %in% ("consensuscluster")]
train_label  <- as.data.frame(as.numeric(train_classif_orig$consensuscluster))
train_matrix <- xgb.DMatrix(data = as.matrix(train_data), label = as.matrix(train_label-1))

# split test data and make xgb.DMatrix
test_data  <- test_classif_orig[, !names(test_classif_orig) %in% ("consensuscluster")]
test_label <- as.data.frame(as.numeric(test_classif_orig$consensuscluster))
test_matrix <- xgb.DMatrix(data = as.matrix(test_data), label = as.matrix(test_label-1))

# creating a scaler based on the training set
#X_train_scaled = scale(train_data)

# applying the scaler to the test seet
#X_test_scaled = scale(
#  test_data,
#  center = attr(X_train_scaled, "scaled:center"),
#  scale = attr(X_train_scaled, "scaled:scale")
#)
```


K-folds Cross-validation to Estimate Error
```{r}
set.seed(1)

#numberOfClasses <- 2

xgb_params <- list(booster = "gbtree",
                   objective = "binary:logistic")

nround    <- 100 # number of XGBoost rounds
cv.nfold  <- 25

# Fit cv.nfold * cv.nround XGB models and save OOF predictions
cv_model <- xgb.cv(params = xgb_params,
                   data = train_matrix, 
                   nrounds = nround,
                   nfold = cv.nfold,
                   verbose = TRUE,
                   prediction = TRUE,
                   early_stopping_rounds = 20,
                   stratified = TRUE,
                   print_every_n = 10)
```

**Assess Out-of-Fold Prediction Error**
To assess prediction error, we used the data returned from the pred slot of the cv_model object. The pred object contains the predicted value for each observation in our train dataset when it was in the kth fold, known as the out-of-fold sample. This sample was not used to train the model so therefore acts as an independent sample for testing (all caveats about k-folds CV apply!). This step allows us to derived an error estimate on all of our train samples as if the were independent from the model fit.
```{r}
OOF_prediction <- data.frame(cv_model$pred) %>%
  mutate(max_prob = max.col(., ties.method = "last"),
         label = train_label)
head(OOF_prediction)
```

**Confusion Matrix**
```{r}
# confusion matrix
confusionMatrix(factor(OOF_prediction$max_prob),
                factor(as.integer(as.vector(t(OOF_prediction$label)))),
                mode = "everything")
```

**Assess Test Set Error with CV results**
As the errors assessed from the CV routine are acceptable and within a reasonable range of variation (e.g. all folds show relatively consistent error rates), we will proceed to the common practice to then fit the full training data set. 

Following the model fit, the test set test_matrix is predicted using the bst_model. The results are transformed into a matrix of class predictions, the max probability is taken, and the true label is added all in the same way as done above for the OOF data. Of note though is that the prediction is returned as a single vector and needs to be transformed back to a matrix with care. Otherwise, the labels will be jumbled and the confusion matrix will show very poor performance.
```{r}
bst_model <- xgb.train(params = xgb_params,
                       data = train_matrix,
                       nrounds = nround)

numberOfClasses <- 2

# Predict hold-out test set
test_pred <- predict(bst_model, newdata = test_matrix)
test_prediction <- matrix(test_pred, nrow = 1,
                          ncol=length(test_pred)) %>%
  t() %>%
  data.frame() %>%
  mutate(label = test_label,
         max_prob = max.col(., "last"))
# confusion matrix of test set
confusionMatrix(factor(test_prediction$max_prob),
                factor(as.integer(as.vector(t(test_prediction$label)))-1),
                mode = "everything")
```

**Finding the best model: Hyperparameter tuning with Grid Search**

As a final hyperparameter tuning step, it is possible to further improve the performance of our model. Let's proceed to the random / grid search procedure and attempt to find better accuracy. From here on, we'll be using the MLR package for model building. A quick reminder, the MLR package creates its own frame of data, learner as shown below.
```{r}
#loading the library
library(mlr)

# Prepraring the training and test datasets for the Grid Search
train_data_labels <- train_data
train_data_labels$labels <- as.factor(as.matrix(train_label) - 1) 
test_data_labels <- test_data
test_data_labels$labels <- as.factor(as.matrix(test_label) - 1)

#create tasks
traintask <- makeClassifTask(data = train_data_labels, target = "labels")
testtask <- makeClassifTask(data = test_data_labels, target = "labels")
```

Now, we'll set the learner and fix the number of rounds and eta as discussed above.
```{r}
#create learner
lrn <- makeLearner("classif.xgboost", predict.type = "response")
lrn$par.vals <-
  list(
    objective = "binary:logistic",
    eval_metric = "error",
    nrounds = 100L,
    eta = 0.3
  )

#set parameter space
params <-
  makeParamSet(
    makeDiscreteParam("booster", values = c("gbtree", "dart")),
    makeIntegerParam("max_depth", lower = 1L, upper = 10L),
    makeNumericParam("min_child_weight", lower = 1L, upper = 10L),
    makeNumericParam("subsample", lower = 0.5, upper = 1),
    makeNumericParam("colsample_bytree", lower = 0.5, upper = 1)
  )

#set resampling strategy
rdesc <- makeResampleDesc("CV", stratify = T, iters = 10L)
```

With stratify=T, we'll ensure that distribution of target class is maintained in the resampled data sets. If you've noticed above, in the parameter set, I didn't consider gamma for tuning. Simply because during cross validation, we saw that train and test error are in sync with each other. Had either one of them been dragging or rushing, we could have brought this parameter into action.
Now, we'll set the search optimization strategy. Though, xgboost is fast, instead of grid search, we'll use random search to find the best parameters. In random search, we'll build 10 models with different parameters, and choose the one with the least error. You are free to build any number of models.
```{r}
#search strategy
ctrl <- makeTuneControlRandom(maxit = 20L)
```

We'll also set a parallel backend to ensure faster computation. Make sure you've not opened several applications in backend. We'll use all the cores in your machine.
```{r}
#set parallel backend
library(parallel)
library(parallelMap)

parallelStartSocket(cpus = detectCores())

#parameter tuning
mytune <-
  tuneParams(
    learner = lrn,
    task = traintask,
    resampling = rdesc,
    measures = acc,
    par.set = params,
    control = ctrl,
    show.info = T
  )

# Print the accuracy of the tuned model
mytune$y 
```

This newly obtained tuned CV accuracy is better than our default xgboost model. To check the tuning result, write mytune in your R console and press Enter. Let's build a model using tuned parameters and check the final test accuracy.
```{r}
#set hyperparameters
lrn_tune <- setHyperPars(lrn, par.vals = mytune$x)

#train model
xgmodel <- train(learner = lrn_tune, task = traintask)

#predict model
xgpred <- predict(xgmodel, testtask)
```

We've made our predictions on the test set. Let's check our model's accuracy.
```{r}
confusionMatrix(xgpred$data$response, xgpred$data$truth)
```

**Variable Importance**
The final step in this process, and potentially the first step in a the process of understanding the model, is assessing variable importance. Basically, this is a way of using all the splits in the XGBoost trees to understand how how accurate the classifications are based on the splits. This is quantified with the Gain measurement in the variable importance table obtained from the xgb.importance() function. According to the XGBoost documentation.

The convenient xgb.plot.importance() function takes the Gain information and plots it using ggplot2. The variables are also clustered using k-means clustering that optimizes k based on Bayesian Information Criteria (BIC) for k=3. Here we can see that genes FOXA2, CCND1, STX12, MFAP5, TRIM15, and CFHR1 are apparently the most important in classifying each sample to the accuracy demonstrated by bst_model given the data and hyper-parameters.

```{r}
# get the feature real names
names <-  colnames(train_classif_orig[,-1])
# compute feature importance matrix
importance_matrix = xgb.importance(feature_names = names, model = xgmodel$learner.model)
head(importance_matrix)
```

```{r}
# plot
gp = xgb.ggplot.importance(importance_matrix[1:30,], n_clusters = 2)
print(gp) 
```

**Predictions on the validation set (unseen information)**
Pre-processing of validation dataset
```{r}
# split val data and make xgb.DMatrix
val_data   <- temp_val[, !names(temp_val) %in% ("consensuscluster")]
#val_data <- val_data[intersect(bst_model$feature_names, colnames(val_data))]
val_label  <- as.data.frame(as.numeric(temp_val$consensuscluster))
val_matrix <- xgb.DMatrix(data = as.matrix(val_data), label = as.matrix(val_label-1))
```


```{r}
# Predict hold-out test set
val_pred <- predict(xgmodel$learner.model, newdata = val_matrix)


numberOfClasses <- 2

val_prediction <- matrix(val_pred, nrow = 1,
                          ncol=length(val_pred)) %>%
  t() %>%
  data.frame() %>%
  mutate(label = val_label,
         max_prob = max.col(., "last"))
# confusion matrix of test set
confusionMatrix(factor(val_prediction$max_prob),
                factor(as.integer(c(as.vector(t(val_prediction$label-1))[-146]))),
                mode = "everything")
```

### Results of the validation set classification

First, we integrate both labels, consensus clustering and classification cluster labels to the validation genes datasets.
```{r}
# Add the labels to the gene expression validation dataseet
val_data_w_labels <- val_data
val_data_w_labels$pred_clusters <- factor(val_prediction$max_prob)
val_data_w_labels$consensus_clusters <- factor(as.integer(as.vector(t(val_prediction$label-1))))

# Order the rows to match with the original PCA validation dataset
#val_data_w_labels <- val_data_w_labels[row.names(as.data.frame(list(x_val[["x"]][,1:PCs_to_keep_val]))),]
val_data_w_labels <- val_data_w_labels[row.names(as.data.frame(list(x_val[["x"]]))),]

# Add the clustering labels to the PCA validation set
val_data_pca <- do.call("cbind", list(x_val[["x"]][,1:PCs_to_keep_val], as.data.frame(val_data_w_labels$pred_clusters), as.data.frame(val_data_w_labels$consensus_clusters)))
```

Let's review the labels assigned to each of the two validation clusters defined in the clustering step of the pipeline. We will assign the cluster label based on a "voting system", in which the label with the majority of "votes" (frequency) will be selected for each cluster.
```{r}
# Validation set

# Cluster 1 results
table(val_data_w_labels[val_data_w_labels$consensus_clusters == 1,]$pred_clusters)*100/sum(as.numeric(val_data_w_labels[val_data_w_labels$consensus_clusters == 1,]$pred_clusters))

# Cluster 2 results
table(val_data_w_labels[val_data_w_labels$consensus_clusters == 2,]$pred_clusters)*100/sum(as.numeric(val_data_w_labels[val_data_w_labels$consensus_clusters == 2,]$pred_clusters))
```



```{r}
library(ggpubr)
library(dplyr)

train_data_pca <- do.call("cbind", list(x_train[["x"]][,1:PCs_to_keep],as.data.frame(x_train$cluster)))

# Scatter plot on the training dataset
ggscatter(
  train_data_pca,
  x = "PC1",
  y = "PC2",
  color = "x_train$cluster",
  palette = "npg",
  ellipse = TRUE,
  ellipse.type = "convex",
  size = 1.5,
  legend = "right",
  ggtheme = theme_bw(),
  xlab = paste0("Dim 1 (", variance.percent_train[1], "% )"),
  ylab = paste0("Dim 2 (", variance.percent_train[2], "% )")
) +
  stat_mean(aes(color = `x_train$cluster`), size = 4)

# Scatter plot on the validation dataset for the consensus clusters
ggscatter(
  val_data_pca,
  x = "PC1",
  y = "PC2",
  color = "val_data_w_labels$consensus_clusters",
  palette = "npg",
  ellipse = TRUE,
  ellipse.type = "convex",
  size = 1.5,
  legend = "right",
  ggtheme = theme_bw(),
  xlab = paste0("Dim 1 (", variance.percent_val[1], "% )"),
  ylab = paste0("Dim 2 (", variance.percent_val[2], "% )")
) +
  stat_mean(aes(color = `val_data_w_labels$consensus_clusters`), size = 4)

# Scatter plot on the validation dataset for the classification clusters
ggscatter(
  val_data_pca,
  x = "PC1",
  y = "PC2",
  color = "val_data_w_labels$pred_clusters",
  palette = "npg",
  ellipse = TRUE,
  ellipse.type = "convex",
  size = 1.5,
  legend = "right",
  ggtheme = theme_bw(),
  xlab = paste0("Dim 1 (", variance.percent_val[1], "% )"),
  ylab = paste0("Dim 2 (", variance.percent_val[2], "% )")
) +
  stat_mean(aes(color = `val_data_w_labels$pred_clusters`), size = 4)
```

**Clinical information of the validation set**
```{r}
# Order the dataset to match the original clinical dataset
val_data_ordered_clin <- val_data_w_labels[row.names(RNA_clin_data_val),]

# Add the clustering labels to the RNA_clin validation set
val_data_RNA_clin <- do.call("cbind", list(RNA_clin_data_val, as.data.frame(val_data_ordered_clin$pred_clusters), as.data.frame(val_data_ordered_clin$consensus_clusters)))

val_data_RNA_clin$consensuscluster<- val_data_RNA_clin$`val_data_ordered_clin$pred_clusters`
```


**Correlation analysis between validation set and the training set clusters**
```{r}
## Race
x <- t(table(RNA_clin_data[c("consensuscluster","race")]))
y_consensus <- t(table(val_data_RNA_clin[c("val_data_ordered_clin$consensus_clusters","Race")]))
y_classif <- t(table(val_data_RNA_clin[c("val_data_ordered_clin$pred_clusters","Race")]))

print("RACE")
print(x)
print(y_consensus)
print(y_classif)

print("*** C1 T vs C1 V")

print(paste0("Corr. classification results vs training: ", cor(x[c("white", "african american"),"1"], y_classif[c("white", "african american"),"1"], method = "pearson", use = "complete.obs")))
print(paste0("Corr. consensus results vs training: ", cor(x[c("white", "african american"),"1"], y_consensus[c("white", "african american"),"1"], method = "pearson", use = "complete.obs")))
print(paste0("Corr. consensus results vs classification results: ", cor(y_classif[c("white", "african american"),"1"], y_consensus[c("white", "african american"),"1"], method = "pearson", use = "complete.obs")))

print("-----------------------")

print("*** C2 T vs C2 V")

print(paste0("Corr. classification results vs training: ", cor(x[c("white", "african american"),"2"], y_classif[c("white", "african american"),"2"], method = "pearson", use = "complete.obs")))
print(paste0("Corr. consensus results vs training: ", cor(x[c("white", "african american"),"2"], y_consensus[c("white", "african american"),"2"], method = "pearson", use = "complete.obs")))
print(paste0("Corr. consensus results vs classification results: ", cor(y_classif[c("white", "african american"),"2"], y_consensus[c("white", "african american"),"2"], method = "pearson", use = "complete.obs")))

print("-----------------------")

print("*** C3 T vs C3 V")

print(paste0("Corr. classification results vs training: ", cor(x[c("white", "african american"),"3"], y_classif[c("white", "african american"),"3"], method = "pearson", use = "complete.obs")))

print("-----------------------")

## Stage
x <- t(table(RNA_clin_data[c("consensuscluster","tumor_stage")]))
y_consensus <- t(table(val_data_RNA_clin[c("val_data_ordered_clin$consensus_clusters","Stage")]))
y_classif <- t(table(val_data_RNA_clin[c("val_data_ordered_clin$pred_clusters","Stage")]))

print("STAGE")
print(x)
print(y_consensus)
print(y_classif)

print("*** C1 T vs C1 V")

print(paste0("Corr. classification results vs training: ", cor(x[c("1", "2", "3"),"1"], y_classif[c("1", "2", "3"),"1"], method = "pearson", use = "complete.obs")))
print(paste0("Corr. consensus results vs training: ", cor(x[c("1", "2", "3"),"1"], y_consensus[c("1", "2", "3"),"1"], method = "pearson", use = "complete.obs")))
print(paste0("Corr. consensus results vs classification results: ", cor(y_classif[c("1", "2", "3"),"1"], y_consensus[c("1", "2", "3"),"1"], method = "pearson", use = "complete.obs")))

print("-----------------------")

print("*** C2 T vs C2 V")

print(paste0("Corr. classification results vs training: ", cor(x[c("1", "2", "3"),"2"], y_classif[c("1", "2", "3"),"2"], method = "pearson", use = "complete.obs")))
print(paste0("Corr. consensus results vs training: ", cor(x[c("1", "2", "3"),"2"], y_consensus[c("1", "2", "3"),"2"], method = "pearson", use = "complete.obs")))
print(paste0("Corr. consensus results vs classification results: ", cor(y_classif[c("1", "2", "3"),"2"], y_consensus[c("1", "2", "3"),"2"], method = "pearson", use = "complete.obs")))

print("-----------------------")

print("*** C3 T vs C3 V")

print(paste0("Corr. classification results vs training: ", cor(x[c("1", "2", "3"),"3"], y_classif[c("1", "2", "3"),"3"], method = "pearson", use = "complete.obs")))

print("-----------------------")

## Age

# training clinical data
age_c1_consensus <- as.data.frame(do.call(cbind, lapply(RNA_clin_data[RNA_clin_data$consensuscluster=="1",c("consensuscluster","age")], summary)))
age_c2_consensus <- as.data.frame(do.call(cbind, lapply(RNA_clin_data[RNA_clin_data$consensuscluster=="2",c("consensuscluster","age")], summary)))
age_c3_consensus <- as.data.frame(do.call(cbind, lapply(RNA_clin_data[RNA_clin_data$consensuscluster=="3",c("consensuscluster","age")], summary)))
x <- as.data.frame(cbind(age_c1_consensus$age, age_c2_consensus$age, age_c3_consensus$age), row.names = row.names(age_c1_consensus))
x <- x[1:6,]

# validation classification data
age_c1_classif_val <- as.data.frame(do.call(cbind, lapply(val_data_RNA_clin[val_data_RNA_clin$`val_data_ordered_clin$pred_clusters`=="1",c("val_data_ordered_clin$pred_clusters","Age")], summary)))
age_c2_classif_val <- as.data.frame(do.call(cbind, lapply(val_data_RNA_clin[val_data_RNA_clin$`val_data_ordered_clin$pred_clusters`=="2",c("val_data_ordered_clin$pred_clusters","Age")], summary)))
age_c3_classif_val <- as.data.frame(do.call(cbind, lapply(val_data_RNA_clin[val_data_RNA_clin$`val_data_ordered_clin$pred_clusters`=="3",c("val_data_ordered_clin$pred_clusters","Age")], summary)))
y_classif <- as.data.frame(cbind(age_c1_classif_val$Age, age_c2_classif_val$Age, age_c3_classif_val$Age), row.names = row.names(age_c1_classif_val))

# validation consensus data
age_c1_consensus_val <- as.data.frame(do.call(cbind, lapply(val_data_RNA_clin[val_data_RNA_clin$`val_data_ordered_clin$consensus_clusters`=="1",c("val_data_ordered_clin$consensus_clusters","Age")], summary)))
age_c2_consensus_val <- as.data.frame(do.call(cbind, lapply(val_data_RNA_clin[val_data_RNA_clin$`val_data_ordered_clin$consensus_clusters`=="2",c("val_data_ordered_clin$consensus_clusters","Age")], summary)))
age_c3_consensus_val <- as.data.frame(do.call(cbind, lapply(val_data_RNA_clin[val_data_RNA_clin$`val_data_ordered_clin$consensus_clusters`=="3",c("val_data_ordered_clin$consensus_clusters","Age")], summary)))
y_consensus <- as.data.frame(cbind(age_c1_consensus_val$Age, age_c2_consensus_val$Age, age_c3_consensus_val$Age), row.names = row.names(age_c1_consensus_val))


print("AGE")
print(x)
print(y_consensus)
print(y_classif)


print("*** C1 T vs C1 V")

print(paste0("Corr. classification results vs training: ", cor(x[,"V1"], y_classif[,"V1"], method = "pearson", use = "complete.obs")))
print(paste0("Corr. consensus results vs training: ", cor(x[,"V1"], y_consensus[,"V1"], method = "pearson", use = "complete.obs")))
print(paste0("Corr. consensus results vs classification results: ", cor(y_classif[,"V1"], y_consensus[,"V1"], method = "pearson", use = "complete.obs")))

print("-----------------------")

print("*** C2 T vs C2 V")

print(paste0("Corr. classification results vs training: ", cor(x[,"V2"], y_classif[,"V2"], method = "pearson", use = "complete.obs")))
print(paste0("Corr. consensus results vs training: ", cor(x[,"V2"], y_consensus[,"V2"], method = "pearson", use = "complete.obs")))
print(paste0("Corr. consensus results vs classification results: ", cor(y_classif[,"V2"], y_consensus[,"V2"], method = "pearson", use = "complete.obs")))

print("-----------------------")

print("*** C3 T vs C3 V")

print(paste0("Corr. classification results vs training: ", cor(x[,"V3"], y_classif[,"V3"], method = "pearson", use = "complete.obs")))

```


## Consensus clustering and K- Means clustering results

Plotting Consensus Clustering results
```{r}
library(ggpubr)
library(dplyr)

# Training
ggscatter(
    data_train_consensus_clusters,
    x = "PC1",
    y = "PC2",
    color = "consensuscluster",
    palette = "npg",
    ellipse = TRUE,
    ellipse.type = "convex",
    size = 1.5,
    legend = "right",
    ggtheme = theme_bw(),
    xlab = paste0("Dim 1 (", variance.percent_train[1], "% )"),
    ylab = paste0("Dim 2 (", variance.percent_train[2], "% )")
) +
    stat_mean(aes(color = "consensuscluster"), size = 4)

# Validation
# ggscatter(
#     data_val_consensus_clusters,
#     x = "PC1",
#     y = "PC2",
#     color = "consensuscluster",
#     palette = "npg",
#     ellipse = TRUE,
#     ellipse.type = "convex",
#     size = 1.5,
#     legend = "right",
#     ggtheme = theme_bw(),
#     xlab = paste0("Dim 1 (", variance.percent_val[1], "% )"),
#     ylab = paste0("Dim 2 (", variance.percent_val[2], "% )")
# ) +
#     stat_mean(aes(color = "consensuscluster"), size = 4)
```
The overlap among clusters is evident between the PC1 and PC2 after performing K-means clustering. After visual evaluation, K Means clustering annotation seems more delimited. Regardless, we need to take into consideration that conensus clustering is a more robust method that diminishes the biases that k-means property of forcing exclusive clustering may create. Therefore, we will proceed with the consensus clustering annotation.

```{r}
# setting the data frames in a list
list_of_data = list(temp, RNA_clin_data)
# list_of_data_val = list(temp_val, RNA_clin_data_val)

# this will get the intersection of the row.names for everything in the list
common_names = Reduce(intersect, lapply(list_of_data, row.names))
# common_names_val = Reduce(intersect, lapply(list_of_data_val, row.names))

common_names
# common_names_val
```

By looking at the previous plot, we confirm (once again), that the
clustering was not biased due to the difference in data sources and gene
expression techniques.

## Clinical data visualization per cluster

### Plotting categorical and numeric variables per cluster

```{r}
library(glue)
library(ggplot2)

clinical_plots(RNA_clin_data, clinical_vars_names, "counts", 2)
```

```{r}
clinical_plots(RNA_clin_data_val, clinical_vars_names_val, "counts", 4)
```

## Survival Plot

### Survival curves from Kaplan-Meier analysis

The Kaplan--Meier estimator, also known as the product limit estimator,
is a non-parametric statistic used to estimate the survival function
from lifetime data. In medical research, it is often used to measure the
fraction of patients living for a certain amount of time after
treatment.

```{r}
library("survminer")
require("survival")

survival_clin_not_nulls = RNA_clin_data[is.na(RNA_clin_data$Surv_days)!=T & is.na(RNA_clin_data$Surv_event)!=T,c(25,26,27)]

# survival in days
fit_days <-
  survfit(Surv(Surv_days, Surv_event) ~ consensuscluster, data = survival_clin_not_nulls)
```

Plot the survival plot

```{r}
ggsurvplot(fit_days, data = RNA_clin_data, pval = TRUE)
```

Look at the statistical information of the monthly survival plot

```{r}
res.sum <- surv_summary(fit_days)
res.sum
```

### Survival curves from Cox Proportional Hazards Model

```{r}
# Fit Cox Model
cox <-
  coxph(Surv(Surv_days, Surv_event) ~ strata(consensuscluster),
        data = survival_clin_not_nulls)

summary(cox)
```

```{r}
library(simPH)

# Find predicted values
coxFit <- survfit(cox)

# Plot strata in a grid
ggfitStrata(coxFit, byStrata = TRUE, xlab = "Days")

# Plot all in one
ggfitStrata(coxFit, byStrata = FALSE, xlab = "Days")
```

But is there a more systematic way to look at the different covariates?
As you might remember from one of the previous passages, Cox
proportional hazards models allow you to include covariates. You can
build Cox proportional hazards models using the coxph function and
visualize them using the ggforest. These type of plot is called a forest
plot. It shows so-called hazard ratios (HR) which are derived from the
model for all covariates that we included in the formula in coxph.
Briefly, an HR \> 1 indicates an increased risk of death (according to
the definition of h(t)) if a specific condition is met by a patient. An
HR \< 1, on the other hand, indicates a decreased risk.

```{r}
library("ggplot2")
library("survminer")
library("survival")

# Fit a Cox proportional hazards model
fit.coxph <- coxph(Surv(Surv_days, Surv_event) ~ consensuscluster, data = survival_clin_not_nulls)

ggforest(fit.coxph, data = survival_clin_not_nulls)
```

```{r}
summary(fit.coxph)
```

## Significance tests on clinical data

Take a look at the clinical data.
```{r}
RNA_clin_data[0:10, clinical_vars_names] 
RNA_clin_data_val[0:10, clinical_vars_names_val] 
```

```{r}
# Loading the required package for percentage tables
library(RcmdrMisc)
```

**Numeric variables** Running the statistical tests for the numerical variables.

Age
```{r}
clinical_test(
  variable_name = "age",
  clinical_df =  RNA_clin_data,
  formula = consensuscluster ~ age,
  predictor_type = "numerical",
  2
)

clinical_test(
  variable_name = "Age",
  clinical_df =  RNA_clin_data_val,
  formula = consensuscluster ~ Age,
  predictor_type = "numerical",
  4
)
```

For the age, the cluster 1 is significant compared to the other two
clusters, including younger people (mean age \~52) compared to cluster 2
and 3 (mean age \~55).

Tumor size mm
```{r}
clinical_test(
  variable_name = "tumor_size_mm",
  clinical_df =  RNA_clin_data,
  formula = consensuscluster ~ tumor_size_mm,
  predictor_type = "numerical",
  2
)
```

For the tumor size in mm, none of the cluster comparisons show
significance, with all of the means ranging between \~25-30 mm.

relapse_free_months
```{r}
clinical_test(
  variable_name = "relapse_free_months",
  clinical_df =  RNA_clin_data,
  formula = consensuscluster ~ relapse_free_months,
  predictor_type = "numerical",
  2
)
```

For the relapse free months, none of the cluster comparisons show
significance, with a mean ranging from \~95-115 months.

Surv_days
```{r}
clinical_test(
  variable_name = "Surv_days",
  clinical_df =  RNA_clin_data[!is.na(RNA_clin_data[, "Surv_days"]), ],
  formula = consensuscluster ~ Surv_days,
  predictor_type = "numerical",
  2
)
```

For the survival months, the cluster 2 indicates a survival significance
against cluster 1 and 2. The cluster 2 is associated with lower survival
months (mean of \~72 months), compared to cluster 1 (mean of \~97
months) and cluster 3 (mean of \~103 months).

**Categorical variables**

histological_grade
```{r}
clinical_test(
  variable_name = "histological_grade",
  clinical_df =  RNA_clin_data,
  formula = consensuscluster ~ histological_grade,
  predictor_type = "categorical",
  2
)
```

The histological grade shows significance on the non-parametric tests
with p-values \< 0.05, even though the logistic regression model didn't
provide the same results. Visualizing the distributions per cluster, the
cluster 3 shows a higher relative percentage of histological type 2
patients (68%) compared to cluster 1 and 2 (\~10%).

vital_status
```{r}
clinical_test(
  variable_name = "vital_status",
  clinical_df =  RNA_clin_data,
  formula = consensuscluster ~ vital_status,
  predictor_type = "categorical",
  2)
```

The vital status shows significance with p-values \< 0.05. One of the
most significant difference between cluster 2 and cluster 3 is the % of
living patients, where cluster 2 has a higher % of living people (24%)
compared to cluster 3 (\~40%). On the other hand, the most significant
differences between cluster 1 and cluster 2 is the % of living patients
and the % of people that died of disease, where cluster 1 has a higher
chance of dying of disease and of living (in general).

histological_type
```{r}
clinical_test(
  variable_name = "histological_type",
  clinical_df =  RNA_clin_data,
  formula = consensuscluster ~ histological_type,
  predictor_type = "categorical",
  2
)
```

The histological type shows significance with p-values \< 0.05 between
cluster 2 vs 3 and cluster 1 vs 3 on histological type == "Lobular" and
"Mixed". Cluster 3 has a lower percentage of Ductal cases compared to
the other two clusters.

Tumor stage
```{r}
clinical_test(
  variable_name = "tumor_stage",
  clinical_df =  RNA_clin_data,
  formula = consensuscluster ~ tumor_stage,
  predictor_type = "categorical",
  2
)

clinical_test(
  variable_name = "Stage",
  clinical_df =  RNA_clin_data_val,
  formula = consensuscluster ~ Stage,
  predictor_type = "categorical",
  4
)
```

The tumor stage shows significance with p-values \< 0.05 between cluster
1 vs cluster 3. Cluster 3 has a higher percentage of stage 2 cases (68%)
compared to the cluster 3 (47%).

Race
```{r}
clinical_test(
  variable_name = "race",
  clinical_df =  RNA_clin_data,
  formula = consensuscluster ~ race,
  predictor_type = "categorical",
  2
)

clinical_test(
  variable_name = "Race",
  clinical_df =  RNA_clin_data_val,
  formula = consensuscluster ~ Race,
  predictor_type = "categorical",
  4
)
```

Relapse
```{r}
clinical_test(
  variable_name = "Relapse",
  clinical_df =  RNA_clin_data,
  formula = consensuscluster ~ Relapse,
  predictor_type = "categorical",
  2
)
```

The Relapse variable shows no significance among clusters.

menopausal_state
```{r}
clinical_test(
  variable_name = "menopausal_state",
  clinical_df =  RNA_clin_data,
  formula = consensuscluster ~ menopausal_state,
  predictor_type = "categorical",
  2
)
```

The cluster 3 has a higher percentage of post-menopausal patients (75%)
compared to cluster 1 (56%).

chemotherapy
```{r}
clinical_test(
  variable_name = "chemotherapy",
  clinical_df =  RNA_clin_data,
  formula = consensuscluster ~ chemotherapy,
  predictor_type = "categorical",
  2
)
```

The cluster 1 has a higher percentage of patients treated with
chemotherapy (61%) compared to cluster 1 (45%).

radiotherapy
```{r}
clinical_test(
  variable_name = "radiotherapy",
  clinical_df =  RNA_clin_data,
  formula = consensuscluster ~ radiotherapy,
  predictor_type = "categorical",
  2
)
```

The cluster 1 has a higher percentage of patients treated with
radiotherapy (77%) compared to cluster 1 (65%).

hormone_therapy
```{r}
clinical_test(
  variable_name = "hormone_therapy",
  clinical_df =  RNA_clin_data,
  formula = consensuscluster ~ hormone_therapy,
  predictor_type = "categorical",
  2
)
```

There is recorded significance for the hormone therapy values between
cluster 2 vs cluster 3, and cluster 1 vs cluster 3. The cluster 3
presents the highest percentage of patients treated with hormone therapy
(60%) compared to the other two clusters (\~21-26%).

relapse_free_status
```{r}
clinical_test(
  variable_name = "relapse_free_status",
  clinical_df =  RNA_clin_data,
  formula = consensuscluster ~ relapse_free_status,
  predictor_type = "categorical",
  2
)
```

The relapse free status variable shows no significance among clusters.

Surv event
```{r}
clinical_test(
  variable_name = "Surv_event",
  clinical_df =  RNA_clin_data,
  formula = consensuscluster ~ Surv_event,
  predictor_type = "categorical",
  2
)
```

The survival event variable shows no significance among clusters.

## Mutations Analysis

Download the mutations from the available sources.

### Import mutations

```{r}
mut_Ref2 <-
  read.delim(
    file = "~/TNBC_Research/data/mutations_files/Ref2_mutations.txt",
    sep = "\t",
    header = T,
    stringsAsFactors = F,
    skip = 1
  )

print(mut_Ref2[0:10, 0:10])

mut_Ref4 <-
  read.delim(
    file = "~/TNBC_Research/data/mutations_files/Ref4_mutations.txt",
    sep = "\t",
    header = T,
    stringsAsFactors = F,
    skip = 0
  )

print(mut_Ref4[0:10, 0:10])

mut_TCGA <-
  read.delim(
    file = "~/TNBC_Research/data/mutations_files/TCGA_mutations.maf.txt",
    sep = "\t",
    header = T,
    stringsAsFactors = F
  )

print(mut_TCGA[0:10, 0:10])
```

Merging the mutation datasets
```{r}
# Columns in common with all the mutation datasets
merged_mut_cols <- intersect(colnames(mut_Ref2), colnames(mut_Ref4))
merged_mut_cols <- intersect(merged_mut_cols, colnames(mut_TCGA))

# Merged mutation datasets
merged_mut <-
  rbind(mut_Ref2[merged_mut_cols], mut_Ref4[merged_mut_cols])
merged_mut <- rbind(merged_mut, mut_TCGA[merged_mut_cols])

merged_mut[0:10, 0:10]
```

Check the data frame dimensions
```{r}
dim(merged_mut)
```

Defining the cluster groups with the patient IDs
```{r}
library(dplyr)
```

### Running the mutation analysis function

```{r}
mutation_analysis_res <- mutation_analysis(merged_mut, RNA_seq_merged_allgenes, 3, annon_train)
```

### T-test mutations merged

We run a t-test to know which genes change according to the mutations
```{r}
mut_rna_test = mutation_t_test(mutation_analysis_res$mut_matrix_rna, 
                               mutation_analysis_res$rna_mut)
```

Export the data frame with the p-values to a csv file
```{r}
mut_rna_test <- as.data.frame(mut_rna_test)
mut_rna_test$row_names <- rownames(mut_rna_test)

write.csv(
  as.data.frame(mut_rna_test),
  "C:\\Users\\victo\\OneDrive\\Documentos\\TNBC_research\\Clustering\\Mutations_merged_T_test.csv",
  row.names = FALSE
)
```

### Mutations per cluster

```{r}
mut_matrix_rna <- as.data.frame(mut_matrix_rna)
mut_matrix_rna[0:10,0:10]
```

```{r}
RNA_seq_merged <- as.data.frame(t(RNA_seq_merged))
RNA_seq_merged[0:10,0:10]
```

Slice the rnaseq matrices and mutation matrices into the different
clusters
```{r}
# Cluster 1
mut_rnaseq_cluster_res <- create_cluster_rnaseq_matrix(rna_mut, mut_matrix_rna, 1)

mut_matrix_rna_cluster1 <- mut_rnaseq_cluster_res$mut_matrix_rna
mut_rnaseq_match_cluster1 <- mut_rnaseq_cluster_res$mut_rnaseq_match
rna_mut_cluster1 <- mut_rnaseq_cluster_res$rna_mut_cluster

# Cluster 2
mut_rnaseq_cluster_res <- create_cluster_rnaseq_matrix(rna_mut, mut_matrix_rna, 2)

mut_matrix_rna_cluster2 <- mut_rnaseq_cluster_res$mut_matrix_rna
mut_rnaseq_match_cluster2 <- mut_rnaseq_cluster_res$mut_rnaseq_match
rna_mut_cluster2 <- mut_rnaseq_cluster_res$rna_mut_cluster

# Cluster 3
mut_rnaseq_cluster_res <- create_cluster_rnaseq_matrix(rna_mut, mut_matrix_rna, 3)

mut_matrix_rna_cluster3 <- mut_rnaseq_cluster_res$mut_matrix_rna
mut_rnaseq_match_cluster3 <- mut_rnaseq_cluster_res$mut_rnaseq_match
rna_mut_cluster3 <- mut_rnaseq_cluster_res$rna_mut_cluster
```

### Chi2 mutations merged

We run a chi2 to know which genes change according to the mutations from
cluster 1 vs cluster 2

```{r}
# We run a chi2 to know which genes change according to the mutations from cluster 1 vs cluster 2
res12 <- chi2_test_clusters_pair(mut_matrix_rna, mut_matrix_rna_cluster1, mut_matrix_rna_cluster2, c("1","2"))
significant_mutations_values12 <- res12$significant_mutations_values
mut_rna_test_12 <- res12$mut_rna_test
print(significant_mutations_values12)

# We run a chi2 to know which genes change according to the mutations from cluster 1 vs cluster 3
res13 <- chi2_test_clusters_pair(mut_matrix_rna, mut_matrix_rna_cluster1, mut_matrix_rna_cluster3, c("1","3"))
significant_mutations_values13 <- res13$significant_mutations_values
mut_rna_test_13 <- res13$mut_rna_test
print(significant_mutations_values13)

# We run a chi2 to know which genes change according to the mutations from cluster 2 vs cluster 3
res23 <- chi2_test_clusters_pair(mut_matrix_rna, mut_matrix_rna_cluster2, mut_matrix_rna_cluster3, c("2","3"))
significant_mutations_values23 <- res23$significant_mutations_values
mut_rna_test_23 <- res23$mut_rna_test
print(significant_mutations_values23)
```

Export the data frame with the p-values to a csv file

```{r}
mut_rna_test_12 <- as.data.frame(mut_rna_test_12)
mut_rna_test_12$row_names <- rownames(mut_rna_test_12)

mut_rna_test_13 <- as.data.frame(mut_rna_test_13)
mut_rna_test_13$row_names <- rownames(mut_rna_test_13)

mut_rna_test_23 <- as.data.frame(mut_rna_test_23)
mut_rna_test_23$row_names <- rownames(mut_rna_test_23)

write.csv(
  as.data.frame(mut_rna_test_12),
  "~/TNBC_research/code/csv_and_objects/mutations/Mutations_clusters1vs2_Chi2_test.csv",
  row.names = FALSE
)

write.csv(
  as.data.frame(mut_rna_test_13),
  "~/TNBC_research/code/csv_and_objects/mutations/Mutations_clusters1vs3_Chi2_test.csv",
  row.names = FALSE
)

write.csv(
  as.data.frame(mut_rna_test_23),
  "~/TNBC_research/code/csv_and_objects/mutations/Mutations_clusters2vs3_Chi2_test.csv",
  row.names = FALSE
)
```

Comparing the significant mutated genes between both clusters
```{r}
significant_mutations_all <- unique(
  c(
    significant_mutations_names12,
    significant_mutations_names13,
    significant_mutations_names23
  )
)

cc1 <- colSums(t(mut_matrix_rna_cluster1))/
cc2 <- colSums(t(mut_matrix_rna_cluster2))/
cc3 <- colSums(t(mut_matrix_rna_cluster3))/

temp <-
  cbind(cc1[significant_mutations_all], cc2[significant_mutations_all], cc3[significant_mutations_all])

rownames(temp) <- significant_mutations_all
colnames(temp) <- c("cluster1", "cluster2", "cluster3")

print(temp)

plt <-
  barplot(
    t(temp),
    legend.text = c('cluster1', 'cluster2', 'cluster3'),
    xlab = "counts",
    horiz = T,
    las=2
  )
```


## DEGs (Differentially expressed genes among clusters)

TODO: Check the final results of the clusters expressions vs the
resulting DEGs, we expect to have maximum one NA per row.
```{r}
library(limma)

design <- model.matrix( ~ 0 + RNA_clin_data$consensuscluster)
design[0:10, ]
```

```{r}
## the column names are a bit ugly, so we will rename
colnames(design) <- c("Cluster1", "Cluster2")
design[0:10, ]
```

It has been demonstrated that our power to detect differential
expression can be improved if we filter lowly-expressed genes prior to
performing the analysis. Quite how one defines a gene being expressed
may vary from experiment to experiment, so a cut-off that will work for
all datasets is not feasible. Here we consider that around 50% of our
genes will not be expressed, and use the median expression level as a
cut-off.
```{r}
RNA_seq_exprs <-
  RNA_seq_merged_allgenes[which(rownames(RNA_seq_merged_allgenes) != "consensuscluster"),]

RNA_seq_exprs <- RNA_seq_exprs[,!names(RNA_seq_exprs) %in% c(clinical_vars_names, clinical_vars_names_val, "Surv_months")]

RNA_seq_exprs <-
  mutate_all(data.frame(RNA_seq_exprs), function(x)
    as.numeric(as.character(x)))

summary(RNA_seq_exprs[, 1:10])

## calculate median expression level
temp_median <- apply(RNA_seq_exprs, 2, median)
cutoff <- median(temp_median, na.rm = T)

## TRUE or FALSE for whether each gene is "expressed" in each sample
is_expressed <- RNA_seq_exprs > cutoff

## Identify genes expressed in more than 2 samples
keep <- colSums(is_expressed) > 2

## check how many genes are removed / retained.
table(keep)

## subset to just those expressed genes
RNA_seq_exprs <- RNA_seq_exprs[,row.names(as.data.frame(keep))]
```

The lmFit function is used to fit the model to the data. The result of
which is to estimate the expression level in each of the groups that we
specified.
```{r}
RNA_seq_exprs_no_consensuscluster <-
  t(RNA_seq_exprs)[!(row.names(t(RNA_seq_exprs)) %in% c("consensuscluster")), ]

## calculate relative array weights
aw <- arrayWeights(RNA_seq_exprs_no_consensuscluster, design)
aw[0:10]

fit <-
  limma::lmFit(RNA_seq_exprs_no_consensuscluster, design, weights = aw)
fit$coefficients[1:10, ]
```

In order to perform the differential analysis, we have to define the
contrast that we are interested in. In our case we only have two groups
and one contrast of interest. Multiple contrasts can be defined in the
makeContrasts function.
```{r}
contrasts_c1c2 <- makeContrasts(Cluster1 - Cluster2, levels = design)

fitc1c2 <- contrasts.fit(fit, contrasts_c1c2)
```

Finally, apply the empirical Bayes' step to get our differential
expression statistics and p-values.
```{r}
fitc1c2 <- eBayes(fitc1c2)
```

We usually get our first look at the results by using the topTable
command
```{r}
topTable(fitc1c2)
```

If we want to know how many genes are differentially-expressed overall
we can use the decideTests function.

\*\* Cluster1 - Cluster 2
```{r}
decideTests(fitc1c2)
```

```{r}
table(decideTests(fitc1c2))
```

### Further processing and visualisation of DE results

At the moment our results are not particularly easy to navigate as the
only information to identify each gene is the identifier that the
microarray manufacturer has assigned. Fortunately, the GEO entry
contains extensive annotation that we can add. The annotation data can
be retrieved with the fData function and we restrict to columns we are
interested in using select.

For your own data, you will have to choose the columns that are of
interest to you. You probably won't have the same column headings used
here.

Once an annotation data frame has been created, it can be assigned to
our results.
```{r}
full_resultsc1c2 <- topTable(fitc1c2, number = Inf)
full_resultsc1c2 <- tibble::rownames_to_column(full_resultsc1c2, "ID")
```

The "Volcano Plot" function is a common way of visualising the results
of a DE analysis. The x axis shows the log-fold change and the y axis is
some measure of statistical significance, which in this case is the
log-odds, or "B" statistic. A characteristic "volcano" shape should be
seen.

First we create a data frame that we can visualise in ggplot2.
Specifying the number argument to topTable creates a table containing
test results from all genes. We also put the probe IDs as a column
rather than row names.

The flexibility of ggplot2 allows us to automatically label points on
the plot that might be of interest. For example, genes that meet a
particular p-value and log fold-change cut-off. With the code below the
values of p_cutoff and fc_cutoff can be changed as desired.

Furthermore, we can label the identity of some genes. Below we set a
limit of the top "N" genes we want to label, and label each gene
according to it's Symbol.
```{r}
library(ggrepel)

## change according to your needs
p_cutoff <- 0.001
fc_cutoff <- 2
topN <- 20
```

Cluster 1 vs Cluster 2
```{r}
full_resultsc1c2 %>%
  mutate(Significant = p.adjust(P.Value, method="fdr") < p_cutoff, abs(logFC) > fc_cutoff) %>%
  mutate(Rank = 1:n(), Label = ifelse(Rank < topN, ID, "")) %>%
  ggplot(aes(
    x = logFC,
    y = -log10(P.Value),
    col = Significant,
    label = Label
  )) + geom_point() + geom_text_repel(col = "black")
```

We can filter according to p-value (adjusted) and fold-change cut-offs
```{r}
sign_genes_c1c2 <- filter(full_resultsc1c2, adj.P.Val < p_cutoff)
```

We save the results
```{r}
library(readr)
full_resultsc1c2 %>%
  write_csv(file="complete_DEGs_results_c1c2_13_jan_23.csv")
```

Comparing the deferentially expressed genes between all the clusters
```{r}
significant_genes12 <- filter(full_resultsc1c2, p.adjust(P.Value, method="fdr") < p_cutoff)$ID

significant_genes <- unique(significant_genes12)

cc1 <- colMeans(RNA_seq_exprs[RNA_seq_exprs$consensuscluster == 1])
cc2 <- colMeans(RNA_seq_exprs[RNA_seq_exprs$consensuscluster == 2])

temp <-
  cbind(cc1[significant_genes], cc2[significant_genes])

rownames(temp) <- significant_genes
colnames(temp) <- c("cluster1", "cluster2")

temp
```

### Heatmaps of selected genes

**Most differentially-expressed genes** We have already created a table
of differential expression results, which is ranked according to
statistical significance.

To visualise the most differentially-expressed genes, we first need to
extract their ID. These IDs should correspond to rows in the expression
matrix.

In the code below we introduce a new column to the results which just
gives a row number to each gene. We then filter to return data for the
top N results. The pull function is used to extract the ID column as a
variable.
```{r}
## Use to top 20 genes for illustration
topN <- 20

##
ids_of_interest12 <- mutate(full_resultsc1c2, Rank = 1:n()) %>%
  filter(Rank < topN) %>%
  pull(ID)
```

In order to label the heatmap in a useful manner we extract the
corresponding gene symbols.
```{r}
gene_names12 <- mutate(full_resultsc1c2, Rank = 1:n()) %>% 
  filter(Rank < topN) %>% 
  pull(ID)
```

The expression values for the IDs we have retrieved can be obtained by
using the [..] notation to index the expression matrix.
```{r}
## Get the rows corresponding to ids_of_interest and all columns
gene_matrix12 <- t(RNA_seq_exprs)[ids_of_interest12,]
```

We now make the heatmap. A default colour scheme is used, but can be
changed via the arguments.
```{r}
pheatmap(gene_matrix12[,1:20],
     labels_row = gene_names12,
     scale="row")
```

## Classification model

Split the original dataset into stratified train and test sets. The stratification will be made based on the cluster column.
```{r}
# First, install and load the randomForest package
library(randomForest)

## Split the data into a training set and a test set

# First, we identify the intersecting genes between the training and validation gene datasets, we keep only the common genes
common_cols_train_val <- intersect(colnames(RNA_seq_merged_allgenes), colnames(RNA_seq_allgenes_val))

# We filter to keep only the DEGs found in the training dataset
common_cols_train_val_DEGs <- intersect(common_cols_train_val, significant_genes)

classif_train <-
  RNA_seq_merged_allgenes[, colnames(RNA_seq_merged_allgenes) %in% c(common_cols_train_val_DEGs, "consensuscluster")]
classif_val <- RNA_seq_allgenes_val[, colnames(RNA_seq_allgenes_val) %in% c(common_cols_train_val_DEGs)]

# Now, we remove the clinical information to generate the classification model based on gene expression
classif_train_genes <-
  classif_train[, !colnames(classif_train) %in% c(clinical_vars_names, clinical_vars_names_val, "Surv_months")]
classif_val <- classif_val[,!colnames(classif_val) %in% c(clinical_vars_names, clinical_vars_names_val, "Surv_months")]

# Split data into partitions
set.seed(123)
classif_train_split <-
  partition(classif_train_genes$consensuscluster,
            p = c(train = 0.8, test = 0.2))
# We split the origial dataframe based on the split recommendation
train_split <- classif_train_genes[classif_train_split$train,]
test_split <- classif_train_genes[classif_train_split$test,]

# We verify the quantity of samples per cluster of each split
table(train_split$consensuscluster)
table(test_split$consensuscluster)

# Train the random forest
rf_model <- randomForest(consensuscluster ~ ., data = train_split, ntree = 100, mtry = 2)

# Make predictions on the test set
predictions <- predict(rf_model, newdata = test_split)

# Evaluate the performance of the model
library(caret)
confusionMatrix(predictions, test_split$consensuscluster)

```

#### Classifying the validation dataset
```{r}
# Make predictions on the test set
predictions_val <- as.data.frame(predict(rf_model, newdata = classif_val))
colnames(predictions_val) <- c("predicted_clusters")

head(predictions_val)
```

Adding the cluster annotation to the total validation genes data frame
```{r}
# Genes and clustering
RNA_seq_merged_allgenes_val <-
  merge(t(val_mad), predictions_val, by = "row.names", all.x = TRUE)
rownames(RNA_seq_merged_allgenes_val) <-
  t(RNA_seq_merged_allgenes_val["Row.names"])
RNA_seq_merged_allgenes_val = subset(RNA_seq_merged_allgenes_val, select = -c(`Row.names`))

# PCs and clustering
PCs_val <- merge(x_val[["x"]][,1:PCs_to_keep_val], predictions_val, by = "row.names", all.x = TRUE)
rownames(PCs_val) <- t(PCs_val["Row.names"])
PCs_val = subset(PCs_val, select = -c(`Row.names`))

# Clinical variables
RNA_clin_data_val <-
  merge(RNA_clin_data_val, predictions_val, by = "row.names")
rownames(RNA_clin_data_val) <-
  t(RNA_clin_data_val["Row.names"])
RNA_clin_data_val = subset(RNA_clin_data_val, select = -c(`Row.names`))
```

Plotting classification results
```{r}
library(ggpubr)
library(dplyr)

# Add clusters obtained using the K-means algorithm for validation
x_val$cluster <- factor(RNA_clin_data_val$cluster)
eigenvalue_val <- round(get_eigenvalue(x_val), 1)
variance.percent_val <- eigenvalue_val$variance.percent

# Validation
ggscatter(
    PCs_val,
    x = "PC1",
    y = "PC2",
    color = "predicted_clusters",
    palette = "npg",
    ellipse = TRUE,
    ellipse.type = "convex",
    size = 1.5,
    legend = "right",
    ggtheme = theme_bw(),
    xlab = paste0("Dim 1 (", variance.percent_val[1], "% )"),
    ylab = paste0("Dim 2 (", variance.percent_val[2], "% )")
) +
    stat_mean(aes(color = "predicted_clusters"), size = 4)
```

```{r}
library(splitTools)
library(ranger)

# First, we identify the intersecting genes between the training and validation gene datasets, we keep only the common genes
common_cols_train_val <- intersect(colnames(RNA_seq_merged_allgenes), colnames(RNA_seq_allgenes_val))

# We filter to keep only the DEGs found in the training dataset
common_cols_train_val_DEGs <- intersect(common_cols_train_val, significant_genes)

classif_train <- RNA_seq_merged_allgenes[,names(RNA_seq_merged_allgenes) %in% c(common_cols_train_val_DEGs, "consensuscluster")]
classif_val <- RNA_seq_allgenes_val[,names(RNA_seq_allgenes_val) %in% c(common_cols_train_val_DEGs)]

# Now, we remove the clinical information to generate the classification model based on gene expression
classif_train_genes <- classif_train[,!names(classif_train) %in% c(clinical_vars_names, clinical_vars_names_val, "Surv_months")]
classif_val <- classif_val[,!names(classif_val) %in% c(clinical_vars_names, clinical_vars_names_val, "Surv_months")]

# Split data into partitions
set.seed(123)
classif_train_split <- partition(classif_train_genes$consensuscluster, p = c(train = 0.8, test = 0.2))

# We split the origial dataframe based on the split recommendation
train_split <- classif_train_genes[classif_train_split$train, ]
test_split <- classif_train_genes[classif_train_split$test, ]

# We verify the quantity of samples per cluster of each split
table(train_split$consensuscluster)
table(test_split$consensuscluster)
```







We can observe that the ratio of samples per class is consistent in both, the training and test sets.

Pre-processing training data: Standardize predictors after log-transformation.
```{r}
# Standardize predictors after log-transformation.
train_classif <- train_classif_orig[,!names(train_classif_orig) %in% c("consensuscluster")]

# Mean
meanx <- colMeans(train_classif)
one <- rep(1,nrow(train_classif))
normx <- sqrt(drop(one %*% as.matrix(train_classif^2)))
train_classif <- as.data.frame(scale(train_classif, meanx, normx))

# Adding the cluster column
train_classif$cluster <- train_classif_orig$consensuscluster
head(train_classif)

# Compute a marginal relevance measure
a0 <- b0 <- 0
tmp <- train_classif
nx <- dim(train_classif_orig)[2]

for(k in 1:length(table(train_classif$cluster))) {
  tmp1 <- subset(train_classif_orig, train_classif_orig[, nx] == k)
  xc.bar <- colMeans(tmp1[, -nx]) ###average of gene j across class k
  xa.bar <-
    colMeans(tmp[, -nx]) ###average of gene j across all samples
  a0 <- a0 + dim(tmp1)[1] * ((xc.bar - xa.bar) ^ 2)
  b0 <- b0 + colSums((train_classif_orig[, -nx] - xc.bar) ^ 2)
}

bw <- a0/b0
npre <- nx - 1 ### use all genes and ignore bw values
bw1 <- order(bw, decreasing=TRUE)[1:npre]
```

Preprocessing the testing data
```{r}
test_classif <- scale(test_classif_orig[,!names(test_classif_orig) %in% c("consensuscluster")], meanx, normx)[, bw1]
test_classif <- cbind(test_classif, test_classif_orig$consensuscluster)
test_classif <- as.data.frame(test_classif)
```


### Option 1: Using BST
Multi-class HingeBoost is applied for 200 boosting iterations. Plot the evolution
of the misclassification error on the test data versus the iteration counter, as
the multi-class HingeBoost algorithm proceeds while working on the test set.
```{r}
library(bst)

train_classif$cluster <- as.numeric(train_classif$cluster)
test_classif$V630 <- as.numeric(test_classif$V630)

m1 <- m2 <- 200
dat.m1 <- mhingebst(x=train_classif, y=train_classif$cluster, ctrl = bst_control(mstop=m1), family = "hinge", learner = c("ls"))

risk.te1 <- predict(dat.m1, newdata=test_classif, newy=test_classif$V630, mstop=m1, type="error")
plot(risk.te1, type="l", xlab="Iteration", ylab="Test Error")
```
Then, we plot the evolution of the number of genes selected versus the iteration counter.
```{r}
plot(nsel(dat.m1, m1), ylab="Number of PCs", xlab="Iteration", lty="solid", type="l")
```

Multi-class twin HingeBoost is applied based on the results from the first
round HingeBoost for 150 iterations. Plot the evolution of the misclassification error on the test data versus the iteration counter, as the multi-class twin
HingeBoost algorithm proceeds while working on the test set.
```{r}
fhat1 <- predict(dat.m1, mstop=150, type="response")
xinit <- unlist(dat.m1$ensemble[1:150])
xinit <- subset(xinit, !is.na(xinit))

### How many genes selected with mstop=150
length(unique(xinit))
dat.m2 <- mhingebst(x=train_classif, y=train_classif$cluster, ctrl = bst_control(mstop=m2,
twinboost=TRUE, f.init=fhat1, xselect.init=xinit), family = "hinge", learner = "ls")
risk.te1 <- predict(dat.m2, newdata=test_classif, newy=test_classif$V630, mstop=m2, type="error")
plot(risk.te1, type="l", xlab="Iteration", ylab="Test Error")
```
Plot the evolution of the number of genes selected versus the iteration
counter, as the multi-class twin HingeBoost algorithm proceeds while working on the training set.
```{r}
plot(nsel(dat.m2, m2), ylab="Number of PCs", xlab="Iteration", lty="solid", type="l")
```

### Option 2: Using XGBoost

Load packages
```{r}
library("xgboost")  # the main algorithm
library("archdata") # for the sample dataset
library("caret")    # for the confusionmatrix() function (also needs e1071 package)
library("dplyr")    # for some data preperation
library("Ckmeans.1d.dp") # for xgb.ggplot.importance
```

**Preprocess training and test data to feed the XGBoost model**
```{r}
# split train data and make xgb.DMatrix
train_data   <- train_classif_orig[, !names(train_classif_orig) %in% ("consensuscluster")]
train_label  <- as.data.frame(as.numeric(train_classif_orig$consensuscluster))
train_matrix <- xgb.DMatrix(data = as.matrix(train_data), label = as.matrix(train_label-1))

# split test data and make xgb.DMatrix
test_data  <- test_classif_orig[, !names(test_classif_orig) %in% ("consensuscluster")]
test_label <- as.data.frame(as.numeric(test_classif_orig$consensuscluster))
test_matrix <- xgb.DMatrix(data = as.matrix(test_data), label = as.matrix(test_label-1))

# creating a scaler based on the training set
#X_train_scaled = scale(train_data)

# applying the scaler to the test seet
#X_test_scaled = scale(
#  test_data,
#  center = attr(X_train_scaled, "scaled:center"),
#  scale = attr(X_train_scaled, "scaled:scale")
#)
```


K-folds Cross-validation to Estimate Error
```{r}
set.seed(1)

#numberOfClasses <- 2

xgb_params <- list(booster = "gbtree",
                   objective = "binary:logistic")

nround    <- 100 # number of XGBoost rounds
cv.nfold  <- 25

# Fit cv.nfold * cv.nround XGB models and save OOF predictions
cv_model <- xgb.cv(params = xgb_params,
                   data = train_matrix, 
                   nrounds = nround,
                   nfold = cv.nfold,
                   verbose = TRUE,
                   prediction = TRUE,
                   early_stopping_rounds = 20,
                   stratified = TRUE,
                   print_every_n = 10)
```

**Assess Out-of-Fold Prediction Error**
To assess prediction error, we used the data returned from the pred slot of the cv_model object. The pred object contains the predicted value for each observation in our train dataset when it was in the kth fold, known as the out-of-fold sample. This sample was not used to train the model so therefore acts as an independent sample for testing (all caveats about k-folds CV apply!). This step allows us to derived an error estimate on all of our train samples as if the were independent from the model fit.
```{r}
OOF_prediction <- data.frame(cv_model$pred) %>%
  mutate(max_prob = max.col(., ties.method = "last"),
         label = train_label)
head(OOF_prediction)
```

**Confusion Matrix**
```{r}
# confusion matrix
confusionMatrix(factor(OOF_prediction$max_prob),
                factor(as.integer(as.vector(t(OOF_prediction$label)))),
                mode = "everything")
```

**Assess Test Set Error with CV results**
As the errors assessed from the CV routine are acceptable and within a reasonable range of variation (e.g. all folds show relatively consistent error rates), we will proceed to the common practice to then fit the full training data set. 

Following the model fit, the test set test_matrix is predicted using the bst_model. The results are transformed into a matrix of class predictions, the max probability is taken, and the true label is added all in the same way as done above for the OOF data. Of note though is that the prediction is returned as a single vector and needs to be transformed back to a matrix with care. Otherwise, the labels will be jumbled and the confusion matrix will show very poor performance.
```{r}
bst_model <- xgb.train(params = xgb_params,
                       data = train_matrix,
                       nrounds = nround)

numberOfClasses <- 2

# Predict hold-out test set
test_pred <- predict(bst_model, newdata = test_matrix)
test_prediction <- matrix(test_pred, nrow = 1,
                          ncol=length(test_pred)) %>%
  t() %>%
  data.frame() %>%
  mutate(label = test_label,
         max_prob = max.col(., "last"))
# confusion matrix of test set
confusionMatrix(factor(test_prediction$max_prob),
                factor(as.integer(as.vector(t(test_prediction$label)))-1),
                mode = "everything")
```

**Finding the best model: Hyperparameter tuning with Grid Search**

As a final hyperparameter tuning step, it is possible to further improve the performance of our model. Let's proceed to the random / grid search procedure and attempt to find better accuracy. From here on, we'll be using the MLR package for model building. A quick reminder, the MLR package creates its own frame of data, learner as shown below.
```{r}
#loading the library
library(mlr)

# Prepraring the training and test datasets for the Grid Search
train_data_labels <- train_data
train_data_labels$labels <- as.factor(as.matrix(train_label) - 1) 
test_data_labels <- test_data
test_data_labels$labels <- as.factor(as.matrix(test_label) - 1)

#create tasks
traintask <- makeClassifTask(data = train_data_labels, target = "labels")
testtask <- makeClassifTask(data = test_data_labels, target = "labels")
```

Now, we'll set the learner and fix the number of rounds and eta as discussed above.
```{r}
#create learner
lrn <- makeLearner("classif.xgboost", predict.type = "response")
lrn$par.vals <-
  list(
    objective = "binary:logistic",
    eval_metric = "error",
    nrounds = 100L,
    eta = 0.3
  )

#set parameter space
params <-
  makeParamSet(
    makeDiscreteParam("booster", values = c("gbtree", "dart")),
    makeIntegerParam("max_depth", lower = 1L, upper = 10L),
    makeNumericParam("min_child_weight", lower = 1L, upper = 10L),
    makeNumericParam("subsample", lower = 0.5, upper = 1),
    makeNumericParam("colsample_bytree", lower = 0.5, upper = 1)
  )

#set resampling strategy
rdesc <- makeResampleDesc("CV", stratify = T, iters = 10L)
```

With stratify=T, we'll ensure that distribution of target class is maintained in the resampled data sets. If you've noticed above, in the parameter set, I didn't consider gamma for tuning. Simply because during cross validation, we saw that train and test error are in sync with each other. Had either one of them been dragging or rushing, we could have brought this parameter into action.
Now, we'll set the search optimization strategy. Though, xgboost is fast, instead of grid search, we'll use random search to find the best parameters. In random search, we'll build 10 models with different parameters, and choose the one with the least error. You are free to build any number of models.
```{r}
#search strategy
ctrl <- makeTuneControlRandom(maxit = 20L)
```

We'll also set a parallel backend to ensure faster computation. Make sure you've not opened several applications in backend. We'll use all the cores in your machine.
```{r}
#set parallel backend
library(parallel)
library(parallelMap)

parallelStartSocket(cpus = detectCores())

#parameter tuning
mytune <-
  tuneParams(
    learner = lrn,
    task = traintask,
    resampling = rdesc,
    measures = acc,
    par.set = params,
    control = ctrl,
    show.info = T
  )

# Print the accuracy of the tuned model
mytune$y 
```

This newly obtained tuned CV accuracy is better than our default xgboost model. To check the tuning result, write mytune in your R console and press Enter. Let's build a model using tuned parameters and check the final test accuracy.
```{r}
#set hyperparameters
lrn_tune <- setHyperPars(lrn, par.vals = mytune$x)

#train model
xgmodel <- train(learner = lrn_tune, task = traintask)

#predict model
xgpred <- predict(xgmodel, testtask)
```

We've made our predictions on the test set. Let's check our model's accuracy.
```{r}
confusionMatrix(xgpred$data$response, xgpred$data$truth)
```

**Variable Importance**
The final step in this process, and potentially the first step in a the process of understanding the model, is assessing variable importance. Basically, this is a way of using all the splits in the XGBoost trees to understand how how accurate the classifications are based on the splits. This is quantified with the Gain measurement in the variable importance table obtained from the xgb.importance() function. According to the XGBoost documentation.

The convenient xgb.plot.importance() function takes the Gain information and plots it using ggplot2. The variables are also clustered using k-means clustering that optimizes k based on Bayesian Information Criteria (BIC) for k=3. Here we can see that genes FOXA2, CCND1, STX12, MFAP5, TRIM15, and CFHR1 are apparently the most important in classifying each sample to the accuracy demonstrated by bst_model given the data and hyper-parameters.

```{r}
# get the feature real names
names <-  colnames(train_classif_orig[,-1])
# compute feature importance matrix
importance_matrix = xgb.importance(feature_names = names, model = xgmodel$learner.model)
head(importance_matrix)
```

```{r}
# plot
gp = xgb.ggplot.importance(importance_matrix[1:30,], n_clusters = 2)
print(gp) 
```

**Predictions on the validation set (unseen information)**
Pre-processing of validation dataset
```{r}
# split val data and make xgb.DMatrix
val_data   <- temp_val[, !names(temp_val) %in% ("consensuscluster")]
#val_data <- val_data[intersect(bst_model$feature_names, colnames(val_data))]
val_label  <- as.data.frame(as.numeric(temp_val$consensuscluster))
val_matrix <- xgb.DMatrix(data = as.matrix(val_data), label = as.matrix(val_label-1))
```


```{r}
# Predict hold-out test set
val_pred <- predict(xgmodel$learner.model, newdata = val_matrix)


numberOfClasses <- 2

val_prediction <- matrix(val_pred, nrow = 1,
                          ncol=length(val_pred)) %>%
  t() %>%
  data.frame() %>%
  mutate(label = val_label,
         max_prob = max.col(., "last"))
# confusion matrix of test set
confusionMatrix(factor(val_prediction$max_prob),
                factor(as.integer(c(as.vector(t(val_prediction$label-1))[-146]))),
                mode = "everything")
```

### Results of the validation set classification

First, we integrate both labels, consensus clustering and classification cluster labels to the validation genes datasets.
```{r}
# Add the labels to the gene expression validation dataseet
val_data_w_labels <- val_data
val_data_w_labels$pred_clusters <- factor(val_prediction$max_prob)
val_data_w_labels$consensus_clusters <- factor(as.integer(as.vector(t(val_prediction$label-1))))

# Order the rows to match with the original PCA validation dataset
#val_data_w_labels <- val_data_w_labels[row.names(as.data.frame(list(x_val[["x"]][,1:PCs_to_keep_val]))),]
val_data_w_labels <- val_data_w_labels[row.names(as.data.frame(list(x_val[["x"]]))),]

# Add the clustering labels to the PCA validation set
val_data_pca <- do.call("cbind", list(x_val[["x"]][,1:PCs_to_keep_val], as.data.frame(val_data_w_labels$pred_clusters), as.data.frame(val_data_w_labels$consensus_clusters)))
```

Let's review the labels assigned to each of the two validation clusters defined in the clustering step of the pipeline. We will assign the cluster label based on a "voting system", in which the label with the majority of "votes" (frequency) will be selected for each cluster.
```{r}
# Validation set

# Cluster 1 results
table(val_data_w_labels[val_data_w_labels$consensus_clusters == 1,]$pred_clusters)*100/sum(as.numeric(val_data_w_labels[val_data_w_labels$consensus_clusters == 1,]$pred_clusters))

# Cluster 2 results
table(val_data_w_labels[val_data_w_labels$consensus_clusters == 2,]$pred_clusters)*100/sum(as.numeric(val_data_w_labels[val_data_w_labels$consensus_clusters == 2,]$pred_clusters))
```



```{r}
library(ggpubr)
library(dplyr)

train_data_pca <- do.call("cbind", list(x_train[["x"]][,1:PCs_to_keep],as.data.frame(x_train$cluster)))

# Scatter plot on the training dataset
ggscatter(
  train_data_pca,
  x = "PC1",
  y = "PC2",
  color = "x_train$cluster",
  palette = "npg",
  ellipse = TRUE,
  ellipse.type = "convex",
  size = 1.5,
  legend = "right",
  ggtheme = theme_bw(),
  xlab = paste0("Dim 1 (", variance.percent_train[1], "% )"),
  ylab = paste0("Dim 2 (", variance.percent_train[2], "% )")
) +
  stat_mean(aes(color = `x_train$cluster`), size = 4)

# Scatter plot on the validation dataset for the consensus clusters
ggscatter(
  val_data_pca,
  x = "PC1",
  y = "PC2",
  color = "val_data_w_labels$consensus_clusters",
  palette = "npg",
  ellipse = TRUE,
  ellipse.type = "convex",
  size = 1.5,
  legend = "right",
  ggtheme = theme_bw(),
  xlab = paste0("Dim 1 (", variance.percent_val[1], "% )"),
  ylab = paste0("Dim 2 (", variance.percent_val[2], "% )")
) +
  stat_mean(aes(color = `val_data_w_labels$consensus_clusters`), size = 4)

# Scatter plot on the validation dataset for the classification clusters
ggscatter(
  val_data_pca,
  x = "PC1",
  y = "PC2",
  color = "val_data_w_labels$pred_clusters",
  palette = "npg",
  ellipse = TRUE,
  ellipse.type = "convex",
  size = 1.5,
  legend = "right",
  ggtheme = theme_bw(),
  xlab = paste0("Dim 1 (", variance.percent_val[1], "% )"),
  ylab = paste0("Dim 2 (", variance.percent_val[2], "% )")
) +
  stat_mean(aes(color = `val_data_w_labels$pred_clusters`), size = 4)
```

**Clinical information of the validation set**
```{r}
# Order the dataset to match the original clinical dataset
val_data_ordered_clin <- val_data_w_labels[row.names(RNA_clin_data_val),]

# Add the clustering labels to the RNA_clin validation set
val_data_RNA_clin <- do.call("cbind", list(RNA_clin_data_val, as.data.frame(val_data_ordered_clin$pred_clusters), as.data.frame(val_data_ordered_clin$consensus_clusters)))

val_data_RNA_clin$consensuscluster<- val_data_RNA_clin$`val_data_ordered_clin$pred_clusters`
```


**Correlation analysis between validation set and the training set clusters**
```{r}
## Race
x <- t(table(RNA_clin_data[c("consensuscluster","race")]))
y_consensus <- t(table(val_data_RNA_clin[c("val_data_ordered_clin$consensus_clusters","Race")]))
y_classif <- t(table(val_data_RNA_clin[c("val_data_ordered_clin$pred_clusters","Race")]))

print("RACE")
print(x)
print(y_consensus)
print(y_classif)

print("*** C1 T vs C1 V")

print(paste0("Corr. classification results vs training: ", cor(x[c("white", "african american"),"1"], y_classif[c("white", "african american"),"1"], method = "pearson", use = "complete.obs")))
print(paste0("Corr. consensus results vs training: ", cor(x[c("white", "african american"),"1"], y_consensus[c("white", "african american"),"1"], method = "pearson", use = "complete.obs")))
print(paste0("Corr. consensus results vs classification results: ", cor(y_classif[c("white", "african american"),"1"], y_consensus[c("white", "african american"),"1"], method = "pearson", use = "complete.obs")))

print("-----------------------")

print("*** C2 T vs C2 V")

print(paste0("Corr. classification results vs training: ", cor(x[c("white", "african american"),"2"], y_classif[c("white", "african american"),"2"], method = "pearson", use = "complete.obs")))
print(paste0("Corr. consensus results vs training: ", cor(x[c("white", "african american"),"2"], y_consensus[c("white", "african american"),"2"], method = "pearson", use = "complete.obs")))
print(paste0("Corr. consensus results vs classification results: ", cor(y_classif[c("white", "african american"),"2"], y_consensus[c("white", "african american"),"2"], method = "pearson", use = "complete.obs")))

print("-----------------------")

print("*** C3 T vs C3 V")

print(paste0("Corr. classification results vs training: ", cor(x[c("white", "african american"),"3"], y_classif[c("white", "african american"),"3"], method = "pearson", use = "complete.obs")))

print("-----------------------")

## Stage
x <- t(table(RNA_clin_data[c("consensuscluster","tumor_stage")]))
y_consensus <- t(table(val_data_RNA_clin[c("val_data_ordered_clin$consensus_clusters","Stage")]))
y_classif <- t(table(val_data_RNA_clin[c("val_data_ordered_clin$pred_clusters","Stage")]))

print("STAGE")
print(x)
print(y_consensus)
print(y_classif)

print("*** C1 T vs C1 V")

print(paste0("Corr. classification results vs training: ", cor(x[c("1", "2", "3"),"1"], y_classif[c("1", "2", "3"),"1"], method = "pearson", use = "complete.obs")))
print(paste0("Corr. consensus results vs training: ", cor(x[c("1", "2", "3"),"1"], y_consensus[c("1", "2", "3"),"1"], method = "pearson", use = "complete.obs")))
print(paste0("Corr. consensus results vs classification results: ", cor(y_classif[c("1", "2", "3"),"1"], y_consensus[c("1", "2", "3"),"1"], method = "pearson", use = "complete.obs")))

print("-----------------------")

print("*** C2 T vs C2 V")

print(paste0("Corr. classification results vs training: ", cor(x[c("1", "2", "3"),"2"], y_classif[c("1", "2", "3"),"2"], method = "pearson", use = "complete.obs")))
print(paste0("Corr. consensus results vs training: ", cor(x[c("1", "2", "3"),"2"], y_consensus[c("1", "2", "3"),"2"], method = "pearson", use = "complete.obs")))
print(paste0("Corr. consensus results vs classification results: ", cor(y_classif[c("1", "2", "3"),"2"], y_consensus[c("1", "2", "3"),"2"], method = "pearson", use = "complete.obs")))

print("-----------------------")

print("*** C3 T vs C3 V")

print(paste0("Corr. classification results vs training: ", cor(x[c("1", "2", "3"),"3"], y_classif[c("1", "2", "3"),"3"], method = "pearson", use = "complete.obs")))

print("-----------------------")

## Age

# training clinical data
age_c1_consensus <- as.data.frame(do.call(cbind, lapply(RNA_clin_data[RNA_clin_data$consensuscluster=="1",c("consensuscluster","age")], summary)))
age_c2_consensus <- as.data.frame(do.call(cbind, lapply(RNA_clin_data[RNA_clin_data$consensuscluster=="2",c("consensuscluster","age")], summary)))
age_c3_consensus <- as.data.frame(do.call(cbind, lapply(RNA_clin_data[RNA_clin_data$consensuscluster=="3",c("consensuscluster","age")], summary)))
x <- as.data.frame(cbind(age_c1_consensus$age, age_c2_consensus$age, age_c3_consensus$age), row.names = row.names(age_c1_consensus))
x <- x[1:6,]

# validation classification data
age_c1_classif_val <- as.data.frame(do.call(cbind, lapply(val_data_RNA_clin[val_data_RNA_clin$`val_data_ordered_clin$pred_clusters`=="1",c("val_data_ordered_clin$pred_clusters","Age")], summary)))
age_c2_classif_val <- as.data.frame(do.call(cbind, lapply(val_data_RNA_clin[val_data_RNA_clin$`val_data_ordered_clin$pred_clusters`=="2",c("val_data_ordered_clin$pred_clusters","Age")], summary)))
age_c3_classif_val <- as.data.frame(do.call(cbind, lapply(val_data_RNA_clin[val_data_RNA_clin$`val_data_ordered_clin$pred_clusters`=="3",c("val_data_ordered_clin$pred_clusters","Age")], summary)))
y_classif <- as.data.frame(cbind(age_c1_classif_val$Age, age_c2_classif_val$Age, age_c3_classif_val$Age), row.names = row.names(age_c1_classif_val))

# validation consensus data
age_c1_consensus_val <- as.data.frame(do.call(cbind, lapply(val_data_RNA_clin[val_data_RNA_clin$`val_data_ordered_clin$consensus_clusters`=="1",c("val_data_ordered_clin$consensus_clusters","Age")], summary)))
age_c2_consensus_val <- as.data.frame(do.call(cbind, lapply(val_data_RNA_clin[val_data_RNA_clin$`val_data_ordered_clin$consensus_clusters`=="2",c("val_data_ordered_clin$consensus_clusters","Age")], summary)))
age_c3_consensus_val <- as.data.frame(do.call(cbind, lapply(val_data_RNA_clin[val_data_RNA_clin$`val_data_ordered_clin$consensus_clusters`=="3",c("val_data_ordered_clin$consensus_clusters","Age")], summary)))
y_consensus <- as.data.frame(cbind(age_c1_consensus_val$Age, age_c2_consensus_val$Age, age_c3_consensus_val$Age), row.names = row.names(age_c1_consensus_val))


print("AGE")
print(x)
print(y_consensus)
print(y_classif)


print("*** C1 T vs C1 V")

print(paste0("Corr. classification results vs training: ", cor(x[,"V1"], y_classif[,"V1"], method = "pearson", use = "complete.obs")))
print(paste0("Corr. consensus results vs training: ", cor(x[,"V1"], y_consensus[,"V1"], method = "pearson", use = "complete.obs")))
print(paste0("Corr. consensus results vs classification results: ", cor(y_classif[,"V1"], y_consensus[,"V1"], method = "pearson", use = "complete.obs")))

print("-----------------------")

print("*** C2 T vs C2 V")

print(paste0("Corr. classification results vs training: ", cor(x[,"V2"], y_classif[,"V2"], method = "pearson", use = "complete.obs")))
print(paste0("Corr. consensus results vs training: ", cor(x[,"V2"], y_consensus[,"V2"], method = "pearson", use = "complete.obs")))
print(paste0("Corr. consensus results vs classification results: ", cor(y_classif[,"V2"], y_consensus[,"V2"], method = "pearson", use = "complete.obs")))

print("-----------------------")

print("*** C3 T vs C3 V")

print(paste0("Corr. classification results vs training: ", cor(x[,"V3"], y_classif[,"V3"], method = "pearson", use = "complete.obs")))

```

## PATHWAYS
### Using the GSVA::gsva method 
First we get the genes set corresponding to the "homo sapiens" specie (human).
```{r}
library(msigdbr)

human_gene_sets = msigdbr(species = "Homo sapiens", category = "H")
head(human_gene_sets)
```

Save the list of all the available gene symbols for the homo sapiens
gene set

```{r}
m_list = human_gene_sets %>% split(x = .$gene_symbol, f = .$gs_name)
m_list
```

Take a look at the first rows of the normalized genes used for the
clustering analysis.

```{r}
head(train_mad)
```

Now, lets explore the pathways for each cluster
```{r}
library(msigdbr)

# all genes
hallmarks_ssgsea_cluster <- GSVA::gsva(as.matrix(train_mad), 
                               gset.idx.list = m_list, 
                               method= "ssgsea",
                               verbose= TRUE)

```

```{r}
## fit the same linear model now to the GSVA enrichment scores
fit_gsea <- lmFit(hallmarks_ssgsea_cluster, design)

## estimate moderated t-statistics
fit_gsea <- eBayes(fit_gsea)
```

Clusters

```{r}
topTable(fit_gsea, coef="Cluster1", sort.by = "logFC")
topTable(fit_gsea, coef="Cluster2", sort.by = "logFC")
```

### Using the msigdbr::gsea method.

```{r}
library(msigdbr)
library(fgsea)

# Load gene expression data
data <- RNA_seq_exprs[,!colnames(RNA_seq_exprs) %in% c("consensuscluster")]

# Create a vector of group labels
group <- RNA_seq_exprs$consensuscluster

# Perform GSEA using msigdbr library

# stats: Ranks resultados de limma del DEGs sobre los genes, basado en el FC (direccion del efecto) * -log10(P value). 
result <- fgsea(pathways = examplePathways, minSize=15, maxSize=500, stats = exampleRanks)

# Print top 5 enriched gene sets
head(result, 5)
```

